The Language of Thought Hypothesis
First published Tue May 28, 2019
The language of thought hypothesis
(LOTH) proposes that
thinking occurs in a mental language. Often called
Mentalese
,
the mental language resembles spoken language in several key respects:
it contains words that can combine into sentences; the words and
sentences are meaningful; and each sentence’s meaning depends in
a systematic way upon the meanings of its component words and the way
those words are combined. For example, there is a Mentalese
word
whale
that denotes whales, and there is a
Mentalese word
mammal
that denotes
mammals. These words can combine into a Mentalese
sentence
whales are mammals
, which means that
whales are mammals. To believe that whales are mammals is to bear an
appropriate psychological relation to this sentence. During a
prototypical deductive inference, I might transform the Mentalese
sentence
whales are mammals
and the Mentalese
sentence
Moby Dick is a whale
into the
Mentalese sentence
Moby Dick is a mammal
. As I
execute the inference, I enter into a succession of mental states that
instantiate those sentences.
LOTH emerged gradually through the writings of Augustine, Boethius,
Thomas Aquinas, John Duns Scotus, and many others. William of Ockham
offered the first systematic treatment in his
Summa Logicae
(c. 1323), which
meticulously analyzed the meaning and structure of Mentalese
expressions. LOTH was quite popular during the late medieval era, but
it slipped from view in the sixteenth and seventeenth centuries. From
that point through the mid-twentieth century, it played little serious
role within theorizing about the mind.
In the 1970s, LOTH underwent a dramatic revival. The watershed was
publication of Jerry Fodor’s
The Language of Thought
(1975). Fodor argued abductively: our current best scientific theories
of psychological activity postulate Mentalese; we therefore have good
reason to accept that Mentalese exists. Fodor’s analysis exerted
tremendous impact. LOTH once again became a focus of discussion, some
supportive and some critical. Debates over the existence and nature of
Mentalese continue to figure prominently within philosophy and
cognitive science. These debates have pivotal importance for our
understanding of how the mind works.
1. Mental Language
1.1 The Representational Theory of Thought
1.2 Compositional Semantics
1.3 Logical Structure
2. Scope of LOTH
3. Mental Computation
4. Arguments for LOTH
4.1 Argument from Cognitive Science Practice
4.2 Argument from the Productivity of Thought
4.3 Argument from the Systematicity of Thought
4.4 Argument from the Systematicity of Thinking
5. The Connectionist Challenge
6. Regress Objections to LOTH
6.1 Learning a Language
6.2 Understanding a Language
7. Naturalizing the Mind
8. Individuation of Mentalese Expressions
Bibliography
Academic Tools
Other Internet Resources
Related Entries
1. Mental Language
What does it mean to posit a mental language? Or to say that thinking
occurs in this language? Just how “language-like” is
Mentalese supposed to be? To address these questions, we will isolate
some core commitments that are widely shared among LOT theorists.
1.1 The Representational Theory of Thought
Folk psychology routinely explains and predicts behavior by citing
mental states, including beliefs, desires, intentions, fears, hopes,
and so on. To explain why Mary walked to the refrigerator, we might
note that she believed there was orange juice in the refrigerator and
wanted to drink orange juice. Mental states such as belief and desire
are called
propositional attitudes
. They can be specified
using locutions of the form
X
believes that
p
.
X
desires that
p
.
X
intends that
p
.
X
fears that
p
.
etc.
By replacing “
p
” with a sentence, we specify the
content of
X
’s mental state. Propositional attitudes have
intentionality
or
aboutness
: they are about a subject matter.
For that reason, they are often called
intentional
states
.
The term “propositional attitude” originates with Russell
(1918–1919 [1985]) and reflects his own preferred analysis: that
propositional attitudes are relations to
propositions
. 
 A proposition is an abstract entity that determines
a
truth-condition
. To illustrate, suppose John believes that
Paris is north of London. Then John’s belief is a relation to
the proposition
that Paris is north of London
, and this
proposition is true iff Paris is north of London. Beyond the thesis
that propositions determine truth-conditions, there is little
agreement about what propositions are like. The literature offers many
options, mainly derived from theories of Frege (1892 [1997]), Russell
(1918–1919 [1985]), and Wittgenstein (1921 [1922]).
Fodor (1981: 177–203; 1987: 16–26) proposes a theory of
propositional attitudes that assigns a central role to
mental
representations
. A
mental representation
is a mental item with
semantic properties
(such as a denotation, or a meaning, or a
truth-condition, etc.). To believe that
p
, or hope that
p
, or intend that
p
, is to bear an appropriate relation
to a mental representation whose meaning is that
p
. For
example, there is a relation belief* between thinkers and mental
representations, where the following biconditional is true no matter
what English sentence one substitutes for “
p
”:
X
believes that
p
iff there is a mental representation
S
such that
X
believes*
S
and
S
means that
p
.
More generally:
(1)
Each propositional
attitude
A
corresponds to a unique psychological relation
A*
, where the following biconditional is true no matter what
sentence one substitutes for “
p
”:
X
As
that
p
iff there is a mental representation
S
such that
X
bears
A
* to
S
and
S
means that
p
.
On this analysis, mental representations are the most direct objects
of propositional attitudes. A propositional attitude inherits its
semantic properties, including its truth-condition, from the mental
representation that is its object.
Proponents of (1) typically invoke
functionalism
to analyze
A
*.
Each psychological relation
A
* is associated with a distinctive
functional role
: a role that
S
plays within your
mental activity just in case you bear
A
* to
S
. When
specifying what it is to believe*
S
, for example, we might
mention how
S
serves as a basis for inferential reasoning, how
it interacts with desires to produce actions, and so on. Precise
functional roles are to be discovered by scientific psychology.
Following Schiffer (1981), it is common to use the term
“belief-box” as a placeholder for the functional role
corresponding to belief*: to believe*
S
is to place
S
in
your belief box. Similarly for “desire-box”, etc.
(1) is compatible with the view that propositional attitudes are
relations to propositions. One might analyze the locution
“
S
means that
p
” as involving a relation
between
S
and a proposition expressed by
S
. It would
then follow that someone who believes*
S
stands in a
psychologically important relation to the proposition expressed by
S
. Fodor (1987: 17) adopts this approach. He combines a
commitment to mental representations with a commitment to
propositions. In contrast, Field (2001: 30–82) declines to
postulate propositions when analyzing “
S
means that
p
”. He posits mental representations with semantic
properties, but he does not posit propositions expressed by the mental
representations.
The distinction between
types and tokens
is crucial for understanding
(1). A mental representation is a repeatable type that can be
instantiated on different occasions. In the current literature, it is
generally assumed that a mental representation’s tokens are
neurological. For present purposes, the key point is that mental
representations are instantiated by
mental events
. Here we
construe the category of
events
broadly so as to include both
occurrences
(e.g., I form an intention to drink orange juice)
and
enduring states
(e.g., my longstanding belief that
Abraham Lincoln was president of the United States). When mental event
e
instantiates representation
S
, we say that
S
is
tokened
and that
e
is a
tokening
of
S
.
For example, if I believe that whales are mammals, then my belief (a
mental event) is a tokening of a mental representation whose meaning
is that whales are mammals.
According to Fodor (1987: 17), thinking consists in chains of mental
events that instantiate mental representations:
(2)
Thought processes
are causal sequences of tokenings of mental
representations.
A paradigm example is deductive inference: I transition from
believing* the premises to believing* the conclusion. The first mental
event (my belief* in the premises) causes the second (my belief* in
the conclusion).
(1) and (2) fit together naturally as a package that one might call
the representational theory of thought
(RTT). RTT postulates
mental representations that serve as the objects of propositional
attitudes and that constitute the domain of thought
 processes.
[
1
]
RTT as stated requires qualification. There is a clear sense in which
you believe that there are no elephants on Jupiter. However, you
probably never considered the question until now. It is not plausible
that your belief box previously contained a mental representation with
the meaning that there are no elephants on Jupiter. Fodor (1987:
20–26) responds to this sort of example by restricting (1) to
core cases
. Core cases are those where the propositional
attitude figures as a causally efficacious episode in a mental
process. Your tacit belief that there are no elephants on Jupiter does
not figure in your reasoning or decision-making, although it can come
to do so if the question becomes salient and you consciously judge
that there are no elephants on Jupiter. So long as the belief remains
tacit, (1) need not apply. In general, Fodor says, an intentional
mental state that is causally efficacious must involve explicit
tokening of an appropriate mental representation. In a slogan:
“No Intentional Causation without Explicit Representation”
(Fodor 1987: 25). Thus, we should not construe (1) as an attempt at
faithfully analyzing informal discourse about propositional attitudes.
Fodor does not seek to replicate folk psychological categories. He
aims to identify mental states that resemble the propositional
attitudes adduced within folk psychology, that play roughly similar
roles in mental activity, and that can support systematic
theorizing.
Dennett’s (1977 [1981]) review of
The Language of
Thought
raises a widely cited objection to RTT:
In a recent conversation with the designer of a chess-playing program
I heard the following criticism of a rival program: “it thinks
it should get its queen out early”. This ascribes a
propositional attitude to the program in a very useful and predictive
way, for as the designer went on to say, one can usefully count on
chasing that queen around the board. But for all the many levels of
explicit representation to be found in that program, nowhere is
anything roughly synonymous with “I should get my queen out
early” explicitly tokened. The level of analysis to which the
designer’s remark belongs describes features of the program that
are, in an entirely innocent way, emergent properties of the
computational processes that have “engineering reality”. I
see no reason to believe that the relation between belief-talk and
psychological talk will be any more direct.
In Dennett’s example, the chess-playing machine does not
explicitly represent that it should get the queen out early, yet in
some sense it acts upon a belief that it should do so. Analogous
examples arise for human cognition. For example, we often follow rules
of deductive inference without explicitly representing the rules.
To assess Dennett’s objection, we must distinguish sharply
between mental representations and rules governing the manipulation of
mental representations (Fodor 1987: 25). RTT does not require that
every such rule be explicitly represented.
Some
rules may be
explicitly represented—we can imagine a reasoning system that
explicitly represents deductive inference rules to which it conforms.
But the rules
need not
be explicitly represented. They may
merely be implicit in the system’s operations. Only when
consultation of a rule figures as a causally efficacious episode in
mental activity does RTT require that the rule be explicitly
represented. Dennett’s chess machine explicitly represents chess
board configurations and perhaps some rules for manipulating chess
pieces. It never consults any rule akin to
Get the Queen out
early
. For that reason, we should not expect that the machine
explicitly represents this rule even if the rule is in some sense
built into the machine’s programming. Similarly, typical
thinkers do not consult inference rules when engaging in deductive
inference. So RTT does not demand that a typical thinker explicitly
represent inference rules, even if she conforms to them and in some
sense tacitly believes that she should conform to them.
1.2 Compositional Semantics
Natural language is
compositional
: complex 
linguistic expressions are
built from simpler linguistic expressions, and the meaning of a
complex expression is a function of the meanings of its constituents
together with the way those constituents are combined.
Compositional semantics
describes in a systematic way how
semantic properties of a complex expression depend upon semantic
properties of its constituents and the way those constituents are
combined. For example, the truth-condition of a conjunction is
determined as follows: the conjunction is true iff both conjuncts are
true.
Historical and contemporary LOT theorists universally agree that
Mentalese is compositional:
Compositionality of mental representations
(COMP)
: Mental representations have a compositional
semantics: complex representations are composed of simple
constituents, and the meaning of a complex representation depends upon
the meanings of its constituents together with the constituency
structure into which those constituents are arranged.
Clearly, mental language and natural language must differ in many
important respects. For example, Mentalese surely does not have a
phonology. It may not have a morphology either. Nevertheless, COMP
articulates a fundamental point of similarity. Just like natural
language, Mentalese contains complex symbols amenable to semantic
analysis.
What is it for one representation to be a “constituent” of
another? According to Fodor (2008: 108), “constituent structure
is a species of the part/whole relation”. Not
all
parts
of a linguistic expression are constituents: “John ran” is
a constituent of “John ran and Mary jumped”, but
“ran and Mary” is not a constituent because it is not
semantically interpretable. The important point for our purposes is
that all constituents are parts. When a complex representation is
tokened, so are its parts. For example,
intending that \(P \amp Q\) requires having a sentence in your
intention box… one of whose parts is a token of the very same
type that’s in the intention box when you intend that \(P\), and
another of whose parts is a token of the very same type that’s
in the intention box when you intend that \(Q\). (Fodor 1987: 139)
More generally: mental event \(e\) instantiates a complex mental
representation only if \(e\) instantiates all of the
representation’s constituent parts. In that sense, \(e\) itself
has internal complexity.
The complexity of mental events figures crucially here, as highlighted
by Fodor in the following passage (1987: 136):
Practically everybody thinks that the objects of intentional states
are in some way complex… [For example], what you believe when
you believe that \(P \amp Q\) is… something composite, whose
elements are—as it might be—the proposition that
P
and the proposition that
Q
. But the (putative) complexity of
the
intentional object
of a mental state does not, of course,
entail the complexity of the mental state itself… LOT claims
that
mental states
—and not just their propositional
objects—
typically have constituent structure
.
Many philosophers, including Frege and Russell, regard propositions as
structured entities. These philosophers apply a part/whole model to
propositions but not necessarily to mental events during which
thinkers entertain propositions. LOTH as developed by Fodor applies
the part/whole model to the mental events themselves:
what’s at issue here is the complexity of mental events and not
merely the complexity of the propositions that are their intentional
objects. (Fodor 1987: 142)
On this approach, a key element of LOTH is the thesis that mental
events have semantically relevant complexity.
Contemporary proponents of LOTH endorse RTT+COMP. Historical
proponents also believed something in the vicinity (Normore 1990,
2009; Panaccio 1999 [2017]), although of course they did not use
modern terminology to formulate their views. We may regard RTT+COMP as
a minimalist formulation of LOTH, bearing in mind that many
philosophers have used the phrase “language of thought
hypothesis” to denote one of the stronger theses discussed
below. As befits a minimalist formulation, RTT+COMP leaves unresolved
numerous questions about the nature, structure, and psychological role
of Mentalese expressions.
1.3 Logical Structure
In practice, LOT theorists usually adopt a more specific view of the
compositional semantics for Mentalese. They claim that Mentalese
expressions have
logical form
(Fodor 2008: 21). More
specifically, they claim that Mentalese contains analogues to the
familiar logical connectives (
and
,
or
,
not
,
if-then
,
some
,
all
,
the
).
Iterative application of logical connectives generates complex
expressions from simpler expressions. The meaning of a logically
complex expression depends upon the meanings of its parts and upon its
logical structure. Thus, LOT theorists usually endorse a doctrine
along the following lines:
Logically structured mental representations
(LOGIC)
: Some mental representations have logical
structure. The compositional semantics for these mental
representations resembles the compositional semantics for logically
structured natural language expressions.
Medieval LOT theorists used syllogistic and propositional logic to
analyze the semantics of Mentalese (King 2005; Normore 1990).
Contemporary proponents instead use the
predicate calculus
,
which was discovered by Frege (1879 [1967]) and whose semantics was
first systematically articulated by Tarski (1933 [1983]). The view is
that Mentalese contains primitive words—including predicates,
singular terms, and logical connectives—and that these words
combine to form complex sentences governed by something like the
semantics of the predicate calculus.
The notion of a
Mentalese word
corresponds roughly to the
intuitive notion of a
concept
. In fact, Fodor (1998: 70)
construes a concept as a Mentalese word together with its denotation.
For example, a thinker has the concept of a cat only if she has in her
repertoire a Mentalese word that denotes cats.
Logical structure is just one possible paradigm for the structure of
mental representations. Human society employs a wide range of
non-sentential representations, including pictures, maps, diagrams,
and graphs. Non-sentential representations typically contain parts
arranged into a compositionally significant structure. In many cases,
it is not obvious that the resulting complex representations have
logical structure. For example, maps do not seem to contain logical
connectives (Fodor 1991: 295; Millikan 1993: 302; Pylyshyn 2003: 424–5). Nor is
it evident that they contain predicates (Camp 2018; Rescorla 2009c),
although some philosophers contend that they do (Blumson 2012; Casati
& Varzi 1999; Kulvicki 2015).
Theorists often posit mental representations that conform to COMP but
that lack logical structure. The British empiricists postulated
ideas
, which they characterized in broadly imagistic terms.
They emphasized that simple ideas can combine to form complex ideas.
They held that the representational import of a complex idea depends
upon the representational import of its parts and the way those parts
are combined. So they accepted COMP or something close to it
(depending on what exactly “constituency” amounts
 to).
[
2
]
They did not say in much detail how compounding of ideas was supposed
to work, but imagistic structure seems to be the paradigm in at least
some passages. LOGIC plays no significant role in their
 writings.
[
3
]
Partly inspired by the British empiricists, Prinz (2002) and Barsalou
(1999) analyze cognition in terms of image-like representations
derived from perception. Armstrong (1973) and Braddon-Mitchell and
Jackson (2007) propose that propositional attitudes are relations not
to mental sentences but to
mental maps
analogous in important
respects to ordinary concrete maps.
One problem facing imagistic and cartographic theories of thought is
that propositional attitudes are often logically complex (e.g., John
believes
that if Plácido Domingo does not sing then either
Gustavo Dudamel will conduct or the concert will be cancelled
).
Images and maps do not seem to support logical operations: the
negation of a map is not a map; the disjunction of two maps is not a
map; similarly for other logical operations; and similarly for images.
Given that images and maps do not support logical operations, theories
that analyze thought in exclusively imagistic or cartographic terms
will struggle to explain logically complex propositional
 attitudes.
[
4
]
There is room here for a pluralist position that allows mental
representations of different kinds: some with logical structure, some
more analogous to pictures, or maps, or diagrams, and so on. The
pluralist position is widespread within cognitive science, which
posits a range of formats for mental representation (Block 1983; Camp 2009; Johnson-Laird
2004: 187; Kosslyn 1980; McDermott 2001: 69; Pinker 2005: 7; Sloman 1978:
144–76). Fodor himself (1975: 184–195) suggests a view on
which imagistic mental representations co-exist alongside, and
interact with, logically structured Mentalese expressions.
Given the prominent role played by logical structure within historical
and contemporary discussion of Mentalese, one might take LOGIC to be
definitive of LOTH. One might insist that mental representations
comprise a mental
language
only if they have logical
structure. We need not evaluate the merits of this terminological
choice.
2. Scope of LOTH
RTT concerns propositional attitudes and the mental processes in which
they figure, such as deductive inference, reasoning, decision-making,
and planning. It does not address perception, motor control,
imagination, dreaming, pattern recognition, linguistic processing, or
any other mental activity distinct from high-level cognition. Hence
the emphasis upon a language of
thought
: a system of mental
representations that underlie thinking, as opposed to perceiving,
imagining, etc. Nevertheless, talk about a mental language generalizes
naturally from high-level cognition to other mental phenomena.
Perception
is a good example. The perceptual system
transforms proximal sensory stimulations (e.g., retinal stimulations)
into perceptual estimates of environmental conditions (e.g., estimates
of shapes, sizes, colors, locations, etc.). Helmholtz (1867 [1925])
proposed that the transition from proximal sensory input to perceptual
estimates features an
unconscious inference
, similar in key
respects to high-level conscious inference yet inaccessible to
consciousness. Helmholtz’s proposal is foundational to
contemporary
perceptual psychology
, which constructs detailed
mathematical models of unconscious perceptual inference (Knill &
Richards 1996; Rescorla 2015). Fodor (1975: 44–55) argues that
this scientific research program presupposes mental representations.
The representations participate in unconscious inferences or
inference-like transitions executed by the perceptual
 system.
[
5
]
Navigation
is another good example. Tolman (1948)
hypothesized that rats navigate using
cognitive maps
: mental
representations that represent the layout of the spatial environment.
The cognitive map hypothesis, advanced during the heyday of
behaviorism, initially encountered great scorn. It remained a fringe
position well into the 1970s, long after the demise of behaviorism.
Eventually, mounting behavioral and neurophysiological evidence won it
many converts (Gallistel 1990; Gallistel & Matzel 2013; Jacobs
& Menzel 2014; O’Keefe & Nadel 1978; Weiner et al.
2011). Although a few researchers remain skeptical (Mackintosh 20002),
there is now a broad consensus that mammals (and possibly even some
insects) navigate using mental representations of spatial layout.
Rescorla (2017b) summarizes the case for cognitive maps and reviews
some of their core properties.
To what extent should we expect perceptual representations and
cognitive maps to resemble the mental representations that figure in
high-level human thought? It is generally agreed that all these mental
representations have compositional structure. For example, the
perceptual system can bind together a representation of shape and a
representation of size to form a complex representation that an object
has a certain shape and size; the representational import of the
complex representation depends in a systematic way upon the
representational import of the component representations. On the other
hand, it is not clear that perceptual representations have anything
resembling
logical
structure, including even predicative
structure (Burge 2010: 540–544; Fodor 2008: 169–195). Nor
is it evident that cognitive maps contain logical connectives or
predicates (Rescorla 2009a, 2009b). Perceptual processing and
non-human navigation certainly do not seem to instantiate mental
processes that would exploit putative logical structure. In
particular, they do not seem to instantiate deductive inference.
These observations provide ammunition for pluralism about
representational format. Pluralists can posit one system of
compositionally structured mental representations for perception,
another for navigation, another for high-level cognition, and so on.
Different representational systems potentially feature different
compositional mechanisms. As indicated in
section 1.3
,
 pluralism figures prominently in contemporary cognitive science.
Pluralists face some pressing questions. Which compositional
mechanisms figure in which psychological domains? Which
representational formats support which mental operations? How do
different representational formats interface with each other? Further
research bridging philosophy and cognitive science is needed to
address such questions.
3. Mental Computation
Modern proponents of LOTH typically endorse the
computational theory of mind
(CTM), which claims that the mind is a computational system.
Some authors use the phrase “language of thought
hypothesis” so that it definitionally includes CTM as one
component.
In a seminal contribution, Turing (1936) introduced what is now called
the
Turing machine
: an abstract model
 of an idealized computing
device. A Turing machine contains a central processor, governed by
precise mechanical rules, that manipulates symbols inscribed along a
linear array of memory locations. Impressed by the enormous power of
the Turing machine formalism, many researchers seek to construct
computational models of core mental processes, including reasoning,
decision-making, and problem solving. This enterprise bifurcates into
two main branches. The first branch is
artificial
intelligence
(AI), which aims to build “thinking
machines”. Here the goal is primarily an engineering
one—to build a system that instantiates or at least simulates
thought—without any pretense at capturing how the human mind
works. The second branch,
computational psychology
, aims to
construct computational models of human mental activity. AI and
computational psychology both emerged in the 1960s as crucial elements
in the new interdisciplinary initiative
cognitive science
, which
studies the mind by drawing upon psychology, computer science
(especially AI), linguistics, philosophy, economics (especially game
theory and behavioral economics), anthropology, and neuroscience.
From the 1960s to the early 1980s, computational models offered within
psychology were mainly Turing-style models. These models embody a
viewpoint known as
the classical computational theory of mind
(CCTM). According to CCTM, the mind is a computational system similar
in important respects to a Turing machine, and certain core mental
processes are computations similar in important respects to
computations executed by a Turing machine.
CCTM fits together nicely with RTT+COMP. Turing-style computation
operates over symbols, so any Turing-style mental computations must
operate over mental symbols. The essence of RTT+COMP is postulation of
mental symbols. Fodor (1975, 1981) advocates RTT+COMP+CCTM. He holds
that certain core mental processes are Turing-style computations over
Mentalese expressions.
One can endorse RTT+COMP without endorsing CCTM. By positing a system
of compositionally structured mental representations, one does not
commit oneself to saying that operations over the representations are
computational
. Historical LOT theorists could not even
formulate CCTM, for the simple reason that the Turing formalism had
not been discovered. In the modern era, Harman (1973) and Sellars
(1975) endorse something like RTT+COMP but not CCTM. Horgan and
Tienson (1996) endorse RTT+COMP+CTM but not
C
CTM, i.e.,
classical
CTM. They favor a version of CTM grounded in
connectionism
, an alternative
 computational framework that differs
quite significantly from Turing’s approach. Thus, proponents of
RTT+COMP need not accept that mental activity instantiates
Turing-style computation.
Fodor (1981) combines RTT+COMP+CCTM with a view that one might call
the formal-syntactic conception of computation
(FSC).
According to FSC, computation manipulates symbols in virtue of their
formal syntactic properties but not their semantic properties.
FSC draws inspiration from modern logic, which emphasizes the
formalization
of deductive reasoning. To formalize, we
specify a
formal language
whose component linguistic
expressions are individuated non-semantically (e.g., by their
geometric shapes). We describe the expressions as pieces of formal
syntax, without considering what if anything the expressions mean. We
then specify
inference rules
in syntactic, non-semantic
terms. Well-chosen inference rules will carry true premises to true
conclusions. By combining formalization with Turing-style computation,
we can build a physical machine that manipulates symbols based solely
on the formal syntax of the symbols. If we program the machine to
implement appropriate inference rules, then its syntactic
manipulations will transform true premises into true conclusions.
CCTM+FSC says that the mind is a formal syntactic computing system:
mental activity consists in computation over symbols with formal
syntactic properties; computational transitions are sensitive to the
symbols’ formal syntactic properties but not their semantic
properties. The key term “sensitive” is rather imprecise,
allowing some latitude as to the precise import of CCTM+FSC.
Intuitively, the picture is that a mental symbol’s formal syntax
rather than its semantics determines how mental computation
manipulates it. The mind is a “syntactic engine”.
Fodor (1987: 18–20) argues that CCTM+FSC helps illuminate a
crucial feature of cognition:
semantic coherence
. For the
most part, our thinking does not move randomly from thought to
thought. Rather, thoughts are causally connected in a way that
respects their semantics. For example, deductive inference carries
true beliefs to true beliefs. More generally, thinking tends to
respect epistemic properties such as warrant and degree of
confirmation. In some sense, then, our thinking tends to cohere with
semantic relations among thoughts. How is semantic coherence achieved?
How does our thinking manage to track semantic properties? CCTM+FSC
gives one possible answer. It shows how a physical system operating in
accord with physical laws can execute computations that coherently
track semantic properties. By treating the mind as a syntax-driven
machine, we explain how mental activity achieves semantic coherence.
We thereby answer the question:
How is rationality mechanically
possible
?
Fodor’s argument convinced many researchers that CCTM+FSC
decisively advances our understanding of the mind’s relation to
the physical world. But not everyone agrees that CCTM+FSC adequately
integrates semantics into the causal order. A common worry is that the
formal syntactic picture veers dangerously close to
epiphenomenalism
(Block 1990; Kazez 1994). Pre-theoretically,
semantic properties of mental states seem highly relevant to mental
and behavioral outcomes. For example, if I form an intention to walk
to the grocery store, then the fact that my intention concerns the
grocery store rather than the post office helps explain why I walk to
the grocery store rather than the post office. Burge (2010) and
Peacocke (1994) argue that cognitive science theorizing likewise
assigns causal and explanatory importance to semantic properties. The
worry is that CCTM+FSC cannot accommodate the causal and explanatory
importance of semantic properties because it depicts them as causally
irrelevant: formal syntax, not semantics, drives mental computation
forward. Semantics looks epiphenomenal, with syntax doing all the work
(Stich 1983).
Fodor (1990, 1994) expends considerable energy trying to allay
epiphenomenalist worries. Advancing a detailed theory of the relation
between Mentalese syntax and Mentalese semantics, he insists that FSC
can honor the causal and explanatory relevance of semantic properties.
Fodor’s treatment is widely regarded as problematic (Arjo 1996;
Aydede 1997b, 1998; Aydede & Robbins 2001; Perry 1998; Prinz 2011;
Wakefield 2002), although Rupert (2008) and Schneider (2005) espouse
somewhat similar positions.
Partly in response to epiphenomenalist worries, some authors recommend
that we replace FSC with an alternative
semantic conception
of computation (Block 1990; Burge 2010: 95–101; Figdor 2009;
O’Brien & Opie 2006; Peacocke 1994, 1999; Rescorla 2012a).
Semantic computationalists claim that computational transitions are
sometimes sensitive to semantic properties, perhaps in addition to
syntactic properties. More specifically, semantic computationalists
insist that
mental
computation is sometimes sensitive to
semantics. Thus, they reject any suggestion that the mind is a
“syntactic engine” or that mental computation is sensitive
only to formal
 syntax.
[
6
]
To illustrate, consider Mentalese conjunction. This mental symbol
expresses the truth-table for conjunction. According to semantic
computationalists, the symbol’s meaning is relevant (both
causally and explanatorily) to mechanical operations over it. That the
symbol expresses the truth-table for conjunction rather than, say,
disjunction influences the course of computation. We should therefore
reject any suggestion that mental computation is sensitive to the
symbol’s syntactic properties rather than its semantic
properties. The claim is not that mental computation
explicitly
represents
semantic properties of mental symbols. All parties
agree that, in general, it does not. There is no homunculus inside
your head interpreting your mental language. The claim is rather that
semantic properties influence how mental computation proceeds.
(Compare: the momentum of a baseball thrown at a window causally
influences whether the window breaks, even though the window does not
explicitly represent the baseball’s momentum.)
Proponents of the semantic conception differ as to how exactly they
gloss the core claim that some computations are
“sensitive” to semantic properties. They also differ in
their stance towards CCTM. Block (1990) and Rescorla (2014a) focus upon CCTM. They argue
that a symbol’s semantic properties can impact mechanical
operations executed by a Turing-style computational system. In
contrast, O’Brien and Opie (2006) favor connectionism over
CCTM.
Theorists who reject FSC must reject Fodor’s explanation of
semantic coherence. What alternative explanation might they offer? So
far, the question has received relatively little attention. Rescorla
(2017a) argues that semantic computationalists can explain semantic
coherence and simultaneously avoid epiphenomenalist worries by
invoking neural implementation of semantically-sensitive mental
computations.
Fodor’s exposition sometimes suggests that CTM, CCTM, or
CCTM+FSC is definitive of LOTH (1981: 26). Yet not everyone who
endorses RTT+COMP endorses CTM, CCTM, or FSC. One can postulate a
mental language without agreeing that mental activity is
computational, and one can postulate mental computations over a mental
language without agreeing that the computations are sensitive only to
syntactic properties. For most purposes, it is not important whether
we regard CTM, CCTM, or CCTM+FSC as definitive of LOTH. More important
is that we track the distinctions among the doctrines.
4. Arguments for LOTH
The literature offers many arguments for LOTH. This section introduces
four influential arguments, each of which supports LOTH abductively by
citing its explanatory benefits.
Section 5
discusses some prominent objections to the four arguments.
4.1 Argument from Cognitive Science Practice
Fodor (1975) defends RTT+COMP+CCTM by appealing to scientific
practice: our best cognitive science postulates Turing-style mental
computations over Mentalese expressions; therefore, we should accept
that mental computation operates over Mentalese expressions. Fodor
develops his argument by examining detailed case studies, including
perception, decision-making, and linguistic comprehension. He argues
that, in each case, computation over mental representations plays a
central explanatory role. Fodor’s argument was widely heralded
as a compelling analysis of then-current cognitive science.
When evaluating cognitive science support for LOTH, it is crucial to
specify what version of LOTH one has in mind. Specifically,
establishing that certain mental processes operate over mental
representations is not enough to establish RTT. For example, one might
accept that mental representations figure in perception and animal
navigation but not in high-level human cognition. Gallistel and King
(2009) defend COMP+CCTM+FSC through a number of (mainly non-human)
empirical case studies, but they do not endorse RTT. They focus on
relatively low-level phenomena, such as animal navigation, without
discussing human decision-making, deductive inference, problem
solving, or other high-level cognitive phenomena.
4.2 Argument from the Productivity of Thought
During your lifetime, you will only entertain a finite number of
thoughts. In principle, though, there are infinitely many thoughts you
might entertain. Consider:
Mary gave the test tube to John’s daughter.
Mary gave the test tube to John’s daughter’s daughter.
Mary gave the test tube to John’s daughter’s
daughter’s daughter.
⋮
The moral usually drawn is that you have the
competence
to
entertain a potential infinity of thoughts, even though your
performance
is bounded by biological limits upon memory,
attention, processing capacity, and so on. In a slogan: thought is
productive
.
RTT+COMP straightforwardly explains productivity. We postulate a
finite base of primitive Mentalese symbols, along with operations for
combining simple expressions into complex expressions. Iterative
application of the compounding operations generates an infinite array
of mental sentences, each in principle within your cognitive
repertoire. By tokening a mental sentence, you entertain the thought
expressed by it. This explanation leverages the recursive nature of
compositional mechanisms to generate infinitely many expressions from
a finite base. It thereby illuminates how finite creatures such as
ourselves are able to entertain a potential infinity of thoughts.
Fodor and Pylyshyn (1988) argue that, since RTT+COMP provides a
satisfying explanation for productivity, we have good reason to accept
RTT+COMP. A potential worry about this argument is that it rests upon
an infinitary competence never manifested within actual performance.
One might dismiss the supposed infinitary competence as an
idealization that, while perhaps convenient for certain purposes, does
not stand in need of explanation.
4.3 Argument from the Systematicity of Thought
There are systematic interrelations among the thoughts a thinker can
entertain. For example, if you can entertain the thought that John
loves Mary, then you can also entertain the thought that Mary loves
John. Systematicity looks like a crucial property of human thought and
so demands a principled explanation.
RTT+COMP gives a compelling explanation. According to RTT+COMP, your
ability to entertain the thought that
p
hinges upon your
ability to bear appropriate psychological relations to a Mentalese
sentence
S
whose meaning is that
p
. If you are able to
think that John loves Mary, then your internal system of mental
representations includes a mental sentence
John loves
Mary
, composed of mental words
John
,
loves
, and
Mary
combined in the right way. If you have the capacity to stand in
psychological relation
A
* to
John loves
Mary
, then you also have the capacity to stand in relation
A*
to a distinct mental sentence
Mary loves
John
. The constituent words
John
,
loves
,
and
Mary
make the
same semantic contribution to both mental sentences (
John
denotes John,
loves
denotes the loving relation, and
Mary
denotes
Mary), but the words are arranged in different constituency structures
so that the sentences have different meanings. Whereas
John
loves Mary
means that John loves Mary,
Mary
loves John
means that Mary loves John. By
standing in relation
A
* to the sentence
Mary
loves John
, you entertain the thought that Mary loves John.
Thus, an ability to think that John loves Mary entails an ability to
think that John loves Mary. By comparison, an ability to think that
John loves Mary does not entail an ability to think that whales are
mammals or an ability to think that \(56 + 138 = 194\).
Fodor (1987: 148–153) supports RTT+COMP by citing its ability to
explain systematicity. In contrast with the productivity argument, the
systematicity argument does not depend upon infinitary idealizations
that outstrip finite performance. Note that neither argument provides
any direct support for CTM. Neither argument even mentions
computation.
4.4 Argument from the Systematicity of Thinking
There are systematic interrelations among which inferences a thinker
can draw. For example, if you can infer
p
from
p
and
q
, then you can also infer
m
from
m and
n
. The systematicity of thinking requires explanation. Why is it
that thinkers who can infer
p
from
p
and
q
can also infer
m
from
m
and
n
?
RTT+COMP+CCTM gives a compelling explanation. During an inference from
p and q
to
p
, you transit from believing* mental
sentence \(S_1 \amp S_2\) (which means
that p and q
) to
believing* mental sentence \(S_{1}\) (which means that
p
).
According to CCTM, the transition involves symbol manipulation. A
mechanical operation detaches the conjunct \(S_{1}\) from the
conjunction \(S_1 \amp S_2\). The same mechanical operation is
applicable to a conjunction \(S_{3} \amp S_{4}\) (which means
that
m and n
), corresponding to the inference from
m
and
n
to
n
. An ability to execute the first inference entails
an ability to execute the second, because drawing the inference in
either case corresponds to executing a single uniform mechanical
operation. More generally, logical inference deploys mechanical
operations over structured symbols, and the mechanical operation
corresponding to a given inference pattern (e.g., conjunction
introduction, disjunction elimination, etc.) is applicable to any
premises with the right logical structure. The uniform applicability
of a single mechanical operation across diverse symbols explains
inferential systematicity. Fodor and Pylyshyn (1988) conclude that
inferential systematicity provides reason to accept RTT+COMP+CCTM.
Fodor and Pylyshyn (1988) endorse an additional thesis about the
mechanical operations corresponding to logical transitions. In keeping
with FSC, they claim that the operations are sensitive to formal
syntactic properties but not semantic properties. For example,
conjunction elimination responds to Mentalese conjunction as a piece
of pure formal syntax, much as a computer manipulates items in a
formal language without considering what those items mean.
Semantic computationalists reject FSC. They claim that mental
computation is sometimes sensitive to semantic properties. Semantic
computationalists can agree that drawing an inference involves
executing a mechanical operation over structured symbols, and they can
agree that the same mechanical operation uniformly applies to any
premises with appropriate logical structure. So they can still explain
inferential systematicity. However, they can also say that the
postulated mechanical operation is sensitive to semantic properties.
For example, they can say that conjunction elimination is sensitive to
the meaning of Mentalese conjunction.
In assessing the debate between FSC and semantic computationalism, one
must distinguish between
logical
versus
non-logical
symbols. For present purposes, it is common ground that the meanings
of
non-logical
symbols do not inform logical inference. The
inference from \(S_1 \amp S_2\) to \(S_{1}\) features the same
mechanical operation as the inference from \(S_{3} \amp S_{4}\) to
\(S_{4}\), and this mechanical operation is not sensitive to the
meanings of the conjuncts \(S_{1}\), \(S_{2}\), \(S_{3}\), or
\(S_{4}\). It does not follow that the mechanical operation is
insensitive to the meaning of Mentalese conjunction. The meaning of
conjunction might influence how the logical inference proceeds, even
though the meanings of the conjuncts do not.
5. The Connectionist Challenge
In the 1960s and 1970s, cognitive scientists almost universally
modeled mental activity as rule-governed symbol manipulation. In the
1980s, connectionism gained currency as an alternative computational
framework. Connectionists employ computational models, called
neural networks
, that differ quite significantly from
Turing-style models. There is no central processor. There are no
memory locations for symbols to be inscribed. Instead, there is a
network of
nodes
bearing weighted connections to one another.
During computation, waves of activation spread through the network. A
node’s activation level depends upon the weighted activations of
the nodes to which it is connected. Nodes function somewhat
analogously to neurons, and connections between nodes function
somewhat analogously to synapses. One should receive the
neurophysiological analogy cautiously, as there are numerous important
differences between neural networks and actual neural configurations
in the brain (Bechtel & Abramson 2002: 341–343;
Bermúdez 2010: 237–239; Clark 2014: 87–89; Harnish
2002: 359–362).
Connectionists raise many objections to the classical computational
paradigm (Rumelhart, McClelland, & the PDP Research Group 1986;
Horgan & Tienson 1996; McLaughlin & Warfield 1994; Bechtel
& Abrahamsen 2002), such as that classical systems are not
biologically realistic or that they are unable to model certain
psychological tasks. Classicists in turn launch various arguments
against connectionism. The most famous arguments showcase
productivity, systematicity of thought, and systematicity of thinking.
Fodor and Pylyshyn (1988) argue that these phenomena support classical
CTM over connectionist CTM.
Fodor and Pylyshyn’s argument hinges on the distinction between
eliminative connectionism
and
implementationist
connectionism
(cf. Pinker & Prince 1988). Eliminative
connectionists advance neural networks as a
replacement
for
the Turing-style formalism. They deny that mental computation consists
in rule-governed symbol manipulation. Implementationist connectionists
allow that, in some cases, mental computation may instantiate
rule-governed symbol manipulation. They advance neural networks not to
replace classical computations but rather to model how classical
computations are implemented in the brain. The hope is that, because
neural network computation more closely resembles actual brain
activity, it can illuminate the physical realization of rule-governed
symbol manipulation.
Building on Aydede’s (2015) discussion, we may reconstruct Fodor
and Pylyshyn’s argument like so:
Representational mental states and processes exist. An
explanatorily adequate account of cognition should acknowledge these
states and processes.
The representational states and processes that figure in
high-level cognition have certain fundamental properties: thought is
productive
and
systematic
; inferential thinking is
systematic
. The states and processes have these properties as
a matter of
nomic necessity
: it is a psychological law that
they have the properties.
A theory of mental computation is explanatorily adequate only if
it explains the nomic necessity of systematicity and
productivity.
The only way to explain the nomic necessity of systematicity and
productivity is to postulate that high-level cognition instantiates
computation over mental symbols with a compositional semantics.
Specifically, we must accept RTT+COMP.
Either a connectionist theory endorses RTT+COMP or it does
not.
If it does, then it is a version of implementationist
connectionism.
If it does not, then it is a version of eliminative
connectionism. As per (iv), it does not explain productivity and
systematicity. As per (iii), it is not explanatorily adequate.
Conclusion
: Eliminative connectionist theories
are not explanatorily adequate.
The argument does
not
say that neural networks are unable to
model systematicity. One can certainly build a neural network that is
systematic. For example, one might build a neural network that can
represent that John loves Mary only if it can represent that Mary
loves John. The problem is that one might just as well build a neural
network that can represent that John loves Mary but cannot represent
that Mary loves John. Hence, nothing about the connectionist framework
per se
guarantees systematicity. For that reason, the
framework does not explain the nomic necessity of systematicity. It
does not explain why all the minds we find are systematic. In
contrast, the classical framework mandates systematicity, and so it
explains the nomic necessity of systematicity. The only apparent
recourse for connectionists is to adopt the classical explanation,
thereby becoming implementationist rather than eliminative
connectionists.
Fodor and Pylyshyn’s argument has spawned a massive literature,
including too many rebuttals to survey here. The most popular
responses fall into five categories:
Deny (i)
. Some connectionists deny that cognitive
science should posit representational mental states. They believe that
mature scientific theorizing about the mind will delineate
connectionist models specified in non-representational terms (P.S.
Churchland 1986; P.S. Churchland & Sejnowski 1989; P.M. Churchland
1990; P.M. Churchland & P.S. Churchland 1990; Ramsey 2007). If so,
then Fodor and Pylyshyn’s argument falters at its first step.
There is no need to explain why representational mental states are
systematic and productive if one rejects all talk about
representational mental states.
Accept (viii)
. Some authors, such as Marcus (2001), feel that neural networks
are best deployed to illuminate the implementation of Turing-style
models, rather than as replacements for Turing-style models.
Deny (ii).
Some authors claim that Fodor and Pylyshyn
greatly exaggerate the extent to which thought is productive
(Rumelhart & McClelland 1986) or systematic (Dennett 1991; Johnson
2004). Horgan and Tienson (1996: 91–94) question the
systematicity of thinking. They contend that we deviate from norms of
deductive inference more than one would expect if we were following
the rigid mechanical rules postulated by CCTM.
Deny (iv)
. Braddon-Mitchell and Fitzpatrick (1990) offer an evolutionary
explanation for the systematicity of thought, bypassing any appeal to
structured mental representations. In a similar vein, Horgan and
Tienson (1996: 90) seek to explain systematicity by emphasizing how
our survival depends upon our ability to keep track of objects in the
environment and their ever-changing properties. Clark (1991) argues
that systematicity follows from the holistic nature of thought
ascription.
Deny (vi)
. Chalmers (1990, 1993), Smolensky (1991), and
van Gelder (1991) claim that one can reject Turing-style models while
still postulating mental representations with compositionally and
computationally relevant internal structure.
We focus here on (vi).
As discussed in
section 1.2
,
 Fodor elucidates constituency structure in terms of part/whole
relations. A complex representation’s constituents are literal
parts of it. One consequence is that, whenever the first
representation is tokened, so are its constituents. Fodor takes this
consequence to be definitive of classical computation. As Fodor and
McLaughlin (1990: 186) put it:
for a pair of expression types E1, E2, the first is a
Classical
constituent of the second
only if
the
first is tokened whenever the second is tokened.
Thus, structured representations have a
concatenative
structure: each token of a structured representation involves a
concatenation of tokens of the constituent representations.
Connectionists who deny (vi) espouse a
non-concatenative
conception of constituency structure, according to which structure is
encoded by a suitable
distributed representation
.
Developments of the non-concatenative conception are usually quite
technical (Elman 1989; Hinton 1990; Pollack 1990; Smolensky 1990, 1991, 1995; Touretzky 1990).
Most models use
vector
or
tensor algebra
to define
operations over connectionist representations, which are codified by
activity vectors across nodes in a neural network. The representations
are said to have
implicit
constituency structure: the
constituents are not literal parts of the complex representation, but
they can be extracted from the complex representation through suitable
computational operations over it.
Fodor and McLaughlin (1990) grant that distributed representations may
have constituency structure “in an extended sense”. But
they insist that distributed representations are ill-suited to explain
systematicity. They focus especially on the systematicity of thinking,
the classical explanation for which postulates mechanical operations
that respond to constituency structure. Fodor and McLaughlin argue
that the non-concatenative conception cannot replicate the classical
explanation and offers no satisfactory substitute for it. Chalmers
(1993) and Niklasson and van Gelder (1994) disagree. They contend that
a neural network can execute structure-sensitive computations over
representations that have non-concatenative constituency structure.
They conclude that connectionists can explain productivity and
systematicity without retreating to implementationist
connectionism.
Aydede (1995, 1997a) agrees that there is a legitimate notion of
non-concatenative constituency structure, but he questions whether the
resulting models are non-classical. He denies that we should regard
concatenative structure as integral to LOTH. According to Aydede,
concatenative structure is just one possible physical realization of
constituency structure. Non-concatenative structure is another
possible realization. We can accept RTT+COMP without glossing
constituency structure in concatenative terms. On this view, a neural
network whose operations are sensitive to non-concatenative
constituency structure may still count as broadly classical and in
particular as manipulating Mentalese expressions.
The debate between classical and connectionist CTM is still active,
although not as active as during the 1990s. Recent anti-connectionist
arguments tend to have a more empirical flavor. For example, Gallistel
and King (2009) defend CCTM by canvassing a range of non-human
empirical case studies. According to Gallistel and King, the case
studies manifest a kind of productivity that CCTM can easily explain
but eliminative connectionism cannot.
6. Regress Objections to LOTH
LOTH has elicited too many objections to cover in a single
encyclopedia entry. We will discuss two objections, both alleging that
LOTH generates a vicious regress. The first objection emphasizes
language
learning
. The second emphasizes
language
understanding
.
6.1 Learning a Language
Like many cognitive scientists, Fodor holds that children learn a
natural language via
hypothesis formation and testing
.
Children formulate, test, and confirm hypotheses about the denotations
of words. For example, a child learning English will confirm the
hypothesis that “cat” denotes cats. According to Fodor,
denotations are represented in Mentalese. To formulate the hypothesis
that “cat” denotes cats, the child uses a Mentalese word
cat
that denotes cats. It may seem that a regress is now in the
offing, sparked by the question: How does the child learn Mentalese?
Suppose we extend the hypothesis formation and testing model
(henceforth HF) to Mentalese. Then we must posit a meta-language to
express hypotheses about denotations of Mentalese words, a
meta-meta-language to express hypotheses about denotations of
meta-language words, and so on
ad infinitum
(Atherton and
Schwartz 1974: 163).
Fodor responds to the threatened regress by denying we should apply HF
to Mentalese (1975: 65). Children do not test hypotheses about the
denotations of Mentalese words. They do not learn Mentalese at all.
The mental language is
innate
.
The doctrine that
some
concepts are innate was a focal point
in the clash between rationalism versus empiricism. Rationalists
defended the innateness of certain fundamental ideas, such as
god
and
cause
, while empiricists held that all ideas derive from sensory
experience. A major theme in the 1960s cognitive science revolution
was revival of a
nativist
picture, inspired by the
rationalists, on which many key elements of cognition are innate. Most
famously, Chomsky (1965) explained language acquisition by positing
innate knowledge about possible human languages. Fodor’s
innateness thesis was widely perceived as going way beyond all
precedent, verging on the preposterous (P.S. Churchland 1986; Putnam
1988). How could we have an innate ability to represent all the
denotations we mentally represent? For example, how could we innately
possess a Mentalese word
carburetor
that represents carburetors?
In evaluating these issues, it is vital to distinguish between
learning
a concept versus
acquiring
a concept. When
Fodor says that a concept is innate, he does not mean to deny that we
acquire the concept or even that certain kinds of experience are
needed to acquire it. Fodor fully grants that we cannot mentally
represent carburetors at birth and that we come to represent them only
by undergoing appropriate experiences. He agrees that most concepts
are
acquired
. He denies that they are
learned
. In
effect, he uses “innate” as a synonym for
“unlearned” (1975: 96). One might reasonably challenge
Fodor’s usage. One might resist classifying a concept as innate
simply because it is unlearned. However, that is how
Fodor
uses the word “innate”. Properly understood, then,
Fodor’s position is not as far-fetched as it may
 sound.
[
7
]
Fodor gives a simple but striking argument that concepts are
unlearned. The argument begins from the premise that HF is the only
potentially
viable model of concept learning. Fodor then
argues that HF is
not
a viable model of concept learning,
from which he concludes that concepts are unlearned. He offers various
formulations and refinements of the argument over his career. Here is
a relatively recent rendition (2008: 139):
Now, according to HF, the process by which one learns
C
must
include the inductive evaluation of some such hypothesis as “The
C
things are the ones that are green or triangular”. But
the inductive evaluation of that hypothesis itself requires (
inter
alia
) bringing the property
green or triangular
before
the mind as such… Quite generally, you can’t represent
anything as
such and such
unless you already have the concept
such and such
. All that being so, it follows, on pain of
circularity, that “concept learning” as HF understands it
can’t
be a way of acquiring concept
C
…
Conclusion:
If concept learning is as HF understands it, there can
be no such thing
. This conclusion is entirely general; it
doesn’t matter whether the target concept is primitive (like
green
) or complex (like
green
or triangular
).
Fodor’s argument does not presuppose RTT, COMP, or CTM. To the
extent that the argument works, it applies to any view on which people
have concepts.
If concepts are not learned, then how are they acquired? Fodor offers
some preliminary remarks (2008: 144–168), but by his own
admission the remarks are sketchy and leave numerous questions
unanswered (2008: 144–145). Prinz (2011) critiques Fodor’s
positive treatment of concept acquisition.
The most common rejoinder to Fodor’s innateness argument is to
deny that HF is the only viable model of concept learning. The
rejoinder acknowledges that concepts are not learned
through
hypothesis testing
but insists they are learned
through other
means
. Three examples:
Margolis (1998) proposes an acquisition model that differs from
HF but that allegedly yields concept learning. Fodor (2008:
140–144) retorts that Margolis’s model does not yield
genuine concept learning. Margolis and Laurence (2011) insist that it
does.
Carey (2009) maintains that children can “bootstrap”
their way to new concepts using induction, analogical reasoning, and
other techniques. She develops her view in great detail, supporting it
partly through her groundbreaking experimental work with young
children. Fodor (2010) and Rey (2014) object that Carey’s
bootstrapping theory is circular: it surreptitiously presupposes that
children already possess the very concepts whose acquisition it
purports to explain. Beck (2017) and Carey (2014) respond to the circularity
objection.
Shea (2016) argues that connectionist modeling can explain
concept acquisition in non-HF terms and that the resulting models
instantiate genuine learning.
A lot depends here upon what counts as “learning” and what
does not, a question that seems difficult to adjudicate. A closely
connected question is whether concept acquisition is a
rational
process or a mere
causal
process. To the
extent that acquiring some concept is a rational achievement, we will
want to say that one learned the concept. To the extent that acquiring
the concept is a mere causal process (more like catching a cold than
confirming a hypothesis), we will feel less inclined to say that
genuine learning took place (Fodor 1981: 275).
These issues lie at the frontier of psychological and philosophical
research. The key point for present purposes is that there are two
options for halting the regress of language learning: we can say that
thinkers acquire concepts but do not learn them; or we can say that
thinkers learn concepts through some means other than hypothesis
testing. Of course, it is not enough just to note that the two options
exist. Ultimately, one must develop one’s favored option into a
compelling theory. But there is no reason to think that doing so would
reinitiate the regress. In any event, explaining concept acquisition
is an important task facing any theorist who accepts that we have
concepts, whether or not the theorist accepts LOTH. Thus, the learning
regress objection is best regarded not as posing a challenge specific
to LOTH but rather as highlighting a more widely shared theoretical
obligation: the obligation to explain how we acquire concepts.
For further discussion, see the entry on innateness. See also the
exchange between Cowie (1999) and Fodor (2001).
6.2 Understanding a Language
What is it to understand a natural language word? On a popular
picture, understanding a word requires that you mentally represent the
word’s denotation. For example, understanding the word
“cat” requires representing that it denotes cats. LOT
theorists will say that you use Mentalese words to represent
denotations. The question now arises what it is to understand a
Mentalese word. If understanding the Mentalese word requires
representing that it has a certain denotation, then we face an
infinite regress of meta-languages (Blackburn 1984: 43–44).
The standard response is to deny that ordinary thinkers represent
Mentalese words as having denotations (Bach 1987; Fodor 1975:
66–79). Mentalese is not an instrument of communication.
Thinking is not “talking to oneself” in Mentalese. A
typical thinker does not represent, perceive, interpret, or reflect
upon Mentalese expressions. Mentalese serves as a medium within which
her thought occurs, not an object of interpretation. We should not say
that she “understands” Mentalese in the same way that she
understands a natural language.
There is perhaps another sense in which the thinker
“understands” Mentalese: her mental activity coheres with
the meanings of Mentalese words. For example, her deductive reasoning
coheres with the truth-tables expressed by Mentalese logical
connectives. More generally, her mental activity is semantically
coherent. To say that the thinker “understands” Mentalese
in
this
sense is not to say that she represents Mentalese
denotations. Nor is there any evident reason to suspect that
explaining semantic coherence will ultimately require us to posit
mental representation of Mentalese denotations. So there is no regress
of understanding.
For further criticism of this regress argument, see the discussions of
Knowles (1998) and
Laurence and Margolis
 (1997).
[
8
]
7. Naturalizing the Mind
Naturalism
is a movement that seeks to ground philosophical theorizing
in the scientific enterprise. As so often in philosophy, different
authors use the term “naturalism” in different ways. Usage
within philosophy of mind typically connotes an effort to depict
mental states and processes as denizens of the physical world, with no
irreducibly mental entities or properties allowed. In the modern era,
philosophers have often recruited LOTH to advance naturalism. Indeed,
LOTH’s supposed contribution to naturalism is frequently cited
as a significant consideration in its favor. One example is
Fodor’s use of CCTM+FSC to explain semantic coherence. The other
main example turns upon
the problem of intentionality
.
How does intentionality arise? How do mental states come to be
about
anything, or to have semantic properties? Brentano
(1874 [1973: 97]) maintained that intentionality is a hallmark of the
mental as opposed to the physical: “The reference to something
as an object is a distinguishing characteristic of all mental
phenomena. No physical phenomenon exhibits anything similar”. In
response, contemporary naturalists seek to
naturalize
intentionality
. They want to explain in naturalistically
acceptable terms what makes it the case that mental states have
semantic properties. In effect, the goal is to reduce the intentional
to the non-intentional. Beginning in the 1980s, philosophers have
offered various proposals about how to naturalize intentionality. Most
proposals emphasize causal or nomic links between mind and world
(Aydede & Güzeldere 2005; Dretske 1981; Fodor 1987, 1990;
Stalnaker 1984), sometimes also invoking teleological factors
(Millikan 1984, 1993; Neander 2017l; Papineau 1987; Dretske 1988) or
historical lineages of mental states (Devitt 1995; Field 2001).
Another approach,
functional role semantics
, emphasizes the
functional role
of a mental state: the cluster of causal or
inferential relations that the state bears to other mental states. The
idea is that meaning emerges at least partly through these causal and
inferential relations. Some functional role theories cite causal
relations to the external world (Block 1987; Loar 1982), and others do
not (Cummins 1989).
Even the best developed attempts at naturalizing intentionality, such
as Fodor’s (1990) version of the nomic strategy, face serious
problems that no one knows how to solve (M. Greenberg 2014; Loewer
1997). Partly for that reason, the flurry of naturalizing attempts
abated in the 2000s. Burge (2010: 298) reckons that the naturalizing
project is not promising and that current proposals are
“hopeless”. He agrees that we should try to illuminate
representationality by limning its connections to the physical, the
causal, the biological, and the teleological. But he insists that
illumination need not yield a reduction of the intentional to the
non-intentional.
LOTH is neutral as to the naturalization of intentionality. An LOT
theorist might attempt to reduce the intentional to the
non-intentional. Alternatively, she might dismiss the reductive
project as impossible or pointless. Assuming she chooses the reductive
route, LOTH provides guidance regarding how she might proceed.
According to RTT,
X
A
’s that
p
iff there is a mental
representation
S
such that
X
bears
A
* to
S
and
S
means that
p
.
The task of elucidating “
X
A
’s that
p
” in naturalistically acceptable terms factors into two
sub-tasks (Field 2001: 33):
Explain in naturalistically acceptable terms what it is to bear
psychological relation
A
* to mental representation
S
.
Explain in naturalistically acceptable terms what it is for
mental representation
S
to mean that
p
.
As we have seen, functionalism helps with (a). Moreover, COMP provides
a blueprint for tackling (b). We can first delineate a compositional
semantics describing how
S
’s meaning depends upon
semantic properties of its component words and upon the compositional
import of the constituency structure into which those words are
arranged. We can then explain in naturalistically acceptable terms why
the component words have the semantic properties that they have and
why the constituency structure has the compositional import that it
has.
How much does LOTH advance the naturalization of intentionality? Our
compositional semantics for Mentalese may illuminate how the semantic
properties of a complex expression depend upon the semantic properties
of primitive expressions, but it says nothing about how primitive
expressions get their semantic properties in the first place.
Brentano’s challenge (
How could intentionality arise from
purely physical entities and processes?
) remains unanswered. To
meet the challenge, we must invoke naturalizing strategies that go
well beyond LOTH itself, such as the causal or nomic strategies
mentioned above. Those naturalizing strategies are not specifically
linked to LOTH and can usually be tailored to semantic properties of
neural states rather than semantic properties of Mentalese
expressions. Thus, it is debatable how much LOTH ultimately helps us
naturalize intentionality. Naturalizing strategies orthogonal to LOTH
seem to do the heavy lifting.
8. Individuation of Mentalese Expressions
How are Mentalese expressions individuated? Since Mentalese
expressions are types, answering this question requires us to consider
the type/token relation for Mentalese. We want to fill in the
schema
e
and
e
* are tokens of the same Mentalese type iff
R
(
e
,
e
*).
What should we substitute for
R
(
e
,
e
*)? The
literature typically focuses on
primitive
symbol types, and
we will follow suit here.
It is almost universally agreed among contemporary LOT theorists that
Mentalese tokens are neurophysiological entities of some sort. One
might therefore hope to individuate Mentalese types by citing neural
properties of the tokens. Drawing
R
(
e
,
e
*) from
the language of neuroscience induces a theory along the following
lines:
Neural individuation
:
e
and
e
*
are tokens of the same primitive Mentalese type iff
e
and
e
* are tokens of the same neural type.
This schema leaves open how neural types are individuated. We may
bypass that question here, because neural individuation of Mentalese
types finds no proponents in the contemporary literature. The main
reason is that it conflicts with
multiple realizability
: 
 the doctrine that a single mental state type can be realized by
physical systems that are wildly heterogeneous when described in
physical, biological, or neuroscientific terms. Putnam (1967)
introduced multiple realizability as evidence against the
mind/brain identity theory
,
 which asserts that mental state types are brain state
types. Fodor (1975: 13–25) further developed the multiple
realizability argument, presenting it as foundational to
LOTH. Although the multiple realizability argument has subsequently
been challenged (Polger 2004), LOT theorists widely agree that we
should not individuate Mentalese types in neural terms.
The most popular strategy is to individuate Mentalese types
functionally:
Functional individuation
:
e
and
e
* are tokens of the same primitive Mentalese type iff
e
and
e
* have the same functional role.
Field (2001: 56–67), Fodor (1994: 105–109), and Stich (1983:
149–151) pursue functional individuation. They specify
functional roles using a Turing-style computationalism formalism, so
that “functional role” becomes something like
“computational role”, i.e., role within mental
computation.
Functional roles theories divide into two categories:
molecular
and
holist
. Molecular theories isolate
privileged canonical relations that a symbol bears to other symbols.
Canonical relations individuate the symbol, but non-canonical
relations do not. For example, one might individuate Mentalese
conjunction solely through the introduction and elimination rules
governing conjunction while ignoring any other computational rules. If
we say that a symbol’s “canonical functional role”
is constituted by its canonical relations to other symbols, then we
can offer the following theory:
Molecular functional individuation
:
e
and
e
* are tokens of the same primitive Mentalese type iff
e
and
e
* have the same canonical functional role.
One problem facing molecular individuation is that, aside from logical
connectives and a few other special cases, it is difficult to draw any
principled demarcation between canonical and non-canonical relations
(Schneider 2011: 106). Which relations are canonical for
 SOFA?
[
9
]
Citing the demarcation problem, Schneider espouses a holist approach
that individuates mental symbols through
total functional
role
, i.e., every single aspect of the role that a symbol plays
within mental activity:
Holist functional individuation
:
e
and
e
* are tokens of the same primitive Mentalese type iff
e
and
e
* have the same total functional role.
Holist individuation is very fine-grained: the slightest difference in
total functional role entails that different types are tokened. Since
different thinkers will always differ somewhat in their mental
computations, it now looks like two thinkers will never share the same
mental language. This consequence is worrisome, for two reasons
emphasized by Aydede (1998). First, it violates the plausible
publicity
constraint that propositional attitudes are in
principle shareable. Second, it apparently precludes interpersonal
psychological explanations that cite Mentalese expressions. Schneider
(2011: 111–158) addresses both concerns, arguing that they are
misdirected.
A crucial consideration when individuating mental symbols is what role
to assign to semantic properties. Here we may usefully compare
Mentalese with natural language. It is widely agreed that natural
language words do not have their denotations essentially. The English
word “cat” denotes cats, but it could just as well have
denoted dogs, or the number 27, or anything else, or nothing at all,
if our linguistic conventions had been different. Virtually all
contemporary LOT theorists hold that a Mentalese word likewise does
not have its denotation essentially. The Mentalese word
cat
denotes
cats, but it could have had a different denotation had it born
different causal relations to the external world or had it occupied a
different role in mental activity. In that sense,
cat
is a piece of
formal syntax. Fodor’s early view (1981: 225–253) was that
a Mentalese word could have had a
different
denotation but
not an
arbitrarily different
denotation:
cat
could not have
denoted just anything—it could not have denoted the number
27—but it could have denoted some other animal species had the
thinker suitably interacted with that species rather than with cats.
Fodor eventually (1994, 2008) embraces the stronger thesis that a
Mentalese word bears an
arbitrary
relation to its denotation:
cat
could have had any arbitrarily different denotation. Most
contemporary theorists agree (Egan 1992: 446; Field 2001: 58; Harnad
1994: 386; Haugeland 1985: 91: 117–123; Pylyshyn 1984: 50).
The historical literature on LOTH suggests an alternative
semantically permeated
view: Mentalese words are individuated
partly through their denotations. The Mentalese word
cat
is not a
piece of formal syntax subject to reinterpretation. It could not have
denoted another species, or the number 27, or anything else. It
denotes cats
by its inherent nature
. From a semantically
permeated viewpoint, a Mentalese word has its denotation essentially.
Thus, there is a profound difference between natural language and
mental language. Mental words, unlike natural language words, bring
with them one fixed semantic interpretation. The semantically
permeated approach is present in Ockham, among other medieval LOT
theorists (Normore 2003, 2009). In light of the problems facing neural
and functional individuation, Aydede (2005) recommends that we
consider taking semantics into account when individuating Mentalese
expressions. Rescorla (2012b) concurs, defending a semantically permeated approach as
applied to at least some mental representations. He proposes that
certain mental computations operate over mental symbols with essential
semantic properties, and he argues that the proposal fits well with
many sectors of cognitive
 science.
[
10
]
A recurring complaint about the semantically permeated approach is
that inherently meaningful mental representations seem like highly
suspect entities (Putnam 1988: 21). How could a mental word have one
fixed denotation
by its inherent nature
? What magic ensures
the necessary connection between the word and the denotation? These
worries diminish in force if one keeps firmly in mind that Mentalese
words are types. Types are abstract entities corresponding to a scheme
for classifying, or
type-identifying
, tokens. To ascribe a
type to a token is to type-identify the token as belonging to some
category. Semantically permeated types correspond to a classificatory
scheme that takes semantics into account when categorizing tokens. As
Burge emphasizes (2007: 302), there is nothing magical about
semantically-based classification. On the contrary, both folk
psychology and cognitive science routinely classify mental events
based at least partly upon their semantic properties.
A simplistic implementation of the semantically permeated approach
individuates symbol tokens
solely
through their
denotations:
Denotational individuation
:
e
and
e
* are tokens of the same primitive Mentalese type iff
e
and
e
* have the same denotation.
As Aydede (2000) and Schneider (2011) emphasize, denotational
individuation is unsatisfying. Co-referring words may play
significantly different roles in mental activity. Frege’s (1892
[1997]) famous Hesperus-Phosphorus example illustrates: one can
believe that Hesperus is Hesperus without believing that Hesperus is
Phosphorus. As Frege put it, one can think about the same denotation
“in different ways”, or “under different modes of
presentation”. Different modes of presentation have different
roles within mental activity, implicating different psychological
explanations. Thus, a semantically permeated individuative scheme
adequate for psychological explanation must be finer-grained than
denotational individuation allows. It must take mode of presentation
into account. But what it is to think about a denotation “under
the same mode of presentation”? How are “modes of
presentation” individuated? Ultimately, semantically permeated
theorists must grapple with these questions. Rescorla (forthcoming)
offers some suggestions about how to
 proceed.
[
11
]
Chalmers (2012) complains that semantically permeated individuation
sacrifices significant virtues that made LOTH attractive in the first
place. LOTH promised to advance naturalism by grounding cognitive
science in non-representational computational models.
Representationally-specified computational models seem like a
significant retrenchment from these naturalistic ambitions. For
example, semantically permeated theorists cannot accept the FSC
explanation of semantic coherence, because they do not postulate
formal syntactic types manipulated during mental computation.
How compelling one finds naturalistic worries about semantically
permeated individuation will depend on how impressive one finds the
naturalistic contributions made by formal mental syntax. We saw
earlier that FSC arguably engenders a worrisome epiphenomenalism.
Moreover, the semantically permeated approach in no way precludes a
naturalistic reduction of intentionality. It merely precludes invoking
formal syntactic Mentalese types while executing such a reduction. For
example, proponents of the semantically permeated approach can still
pursue the causal or nomic naturalizing strategies discussed in
section 7
.
 Nothing about either strategy presupposes formal syntactic Mentalese
types. Thus, it is not clear that replacing a formal syntactic
individuative scheme with a semantically permeated scheme
significantly impedes the naturalistic endeavor.
No one has yet provided an individuative scheme for Mentalese that
commands widespread assent. The topic demands continued investigation,
because LOTH remains highly schematic until its proponents clarify
sameness and difference of Mentalese types.
Bibliography
Arjo, Dennis, 1996, “Sticking Up for Oedipus: Fodor on
Intentional Generalizations and Broad Content”,
Mind &
Language
, 11(3): 231–245.
doi:10.1111/j.1468-0017.1996.tb00044.x
Armstrong, D. M., 1973,
Belief Truth and Knowledge
,
Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511570827
Atherton, Margaret and Robert Schwartz, 1974, “Linguistic
Innateness and Its Evidence”:,
Journal of Philosophy
,
71(6): 155–168. doi:10.2307/2024657
Aydede, Murat, 1995, “Connectionism and Language of
Thought”, CSLI Technical Report 195, Stanford: Center for the
Study of Language and Information Publications.
–––, 1997a, “Language of Thought: The
Connectionist Contribution”,
Minds and Machines
, 7(1):
57–101. doi:10.1023/A:1008203301671
–––, 1997b, “Has Fodor Really Changed His
Mind on Narrow Content?”,
Mind & Language
,
12(3–4): 422–458. doi:10.1111/j.1468-0017.1997.tb00082.x
–––, 1998, “Fodor on Concepts and Frege
Puzzles”,
Pacific Philosophical Quarterly
, 79(4):
289–294. doi:10.1111/1468-0114.00063
–––, 2000, “On the Type/Token Relation of
Mental Representations”,
Facta Philosophica
, 2:
23–49.
–––, 2005, “Computation and Functionalism:
Syntactic Theory of Mind Revisited”, in
Turkish Studies in
the History and Philosophy of Science
, Gürol Irzik and
Güven Güzeldere (eds.), (Boston Studies in the History and
Philosophy of Science 244), Berlin/Heidelberg: Springer-Verlag,
177–204. doi:10.1007/1-4020-3333-8_13
–––, 2015, “The Language of Thought
Hypothesis”,
The Stanford Encyclopedia of Philosophy
(Fall 2015 Edition), Edward Zalta (ed.). URL =
 <
https://plato.stanford.edu/archives/fall2015/entries/language-thought/
>.
Aydede, Murat and Güven Güzeldere, 2005,
“Cognitive Architecture, Concepts, and Introspection: An
Information-Theoretic Solution to the Problem of Phenomenal
Consciousness”,
Noûs
, 39(2): 197–255.
doi:10.1111/j.0029-4624.2005.00500.x
Aydede, Murat and Philip Robbins, 2001, “Are Frege Cases
Exceptions to Intentional Generalizations?”,
Canadian
Journal of Philosophy
, 31(1): 1–22.
doi:10.1080/00455091.2001.10717558
Bach, Kent, 1987, “Review:
Spreading the
Word
”,
The Philosophical Review
, 96(1): 120–123.
doi:10.2307/2185336
Barsalou, Lawrence W., 1999, “Perceptual Symbol
Systems”,
Behavioral and Brain Sciences
, 22(4):
577–660. doi:10.1017/S0140525X99002149
Bechtel, William and Adele Abrahamsen, 2002,
Connectionism and
the Mind: Parallel Processing, Dynamics and Evolution in
Networks
, second edition, Malden, MA: Blackwell.
Beck, Jacob, 2017, “Can Bootstrapping Explain Concept
Learning?”,
Cognition
, 158: 110–121.
doi:10.1016/j.cognition.2016.10.017
Bermúdez, José Luis, 2010,
Cognitive Science: An
Introduction to the Science of the Mind
, Cambridge: Cambridge
University Press.
Blackburn, Simon, 1984,
Spreading the Word
, Oxford:
Oxford University Press.
Block, Ned, 1983, “Mental Pictures and Cognitive Science”,
The Philosophical Review
, 92(4):
499–451. doi:10.2307/2184879
–––, “Advertisement for a Semantics for
Psychology”, in
Midwest Studies in Philosophy
, 10:
615–678. doi:10.1111/j.1475-4975.1987.tb00558.x
–––, 1990, “Can the Mind Change the
World?”, in
Meaning and Method: Essays in Honor of Hilary
Putnam
, George Boolos (ed.), Cambridge: Cambridge University
Press.
Blumson, Ben, 2012, “Mental Maps”,
Philosophy and
Phenomenological Research
, 85(2): 413–434.
doi:10.1111/j.1933-1592.2011.00499.x
Braddon-Mitchell, David and John Fitzpatrick, 1990, “Explanation and the Language of Thought”,
Synthese
, 83(1): 3–29. doi: 10.1007/BF00413686
Braddon-Mitchell, David and Frank Jackson, 2007,
Philosophy of
Mind and Cognition
, second edition, Cambridge: Blackwell.
Burge, Tyler, 2007,
Foundations of Mind
, (Philosophical
Essays, 2), Oxford: Oxford University Press.
–––, 2010,
Origins of Objectivity
,
Oxford: Oxford University Press.
doi:10.1093/acprof:oso/9780199581405.001.0001
–––, 2018, “Iconic Representation: Maps,
Pictures, and Perception”, in
The Map and the Territory:
Exploring the Foundations of Science, Thought, and Reality
, Shyam
Wuppuluri and Francisco Antonio Doria (eds.), Cham: Springer
International Publishing, 79–100.
doi:10.1007/978-3-319-72478-2_5
Brentano, Franz, 1874 [1973],
Psychology from an Empirical
Standpoint
(
Psychologie vom empirischen Standpunkt
, 1924
edition), Antos C. Rancurello, D.B. Terrell, and Linda McAlister
(trans.), London: Routledge and Kegan Paul.
Camp, Elisabeth, 2009, “A Language of Baboon
Thought?”, in Lurz 2009: 108–127.
doi:10.1017/CBO9780511819001.007
–––, 2018, “Why Maps Are Not
Propositional”, in
Non-Propositional Intentionality
,
Alex Grzankowski and Michelle Montague (eds.), Oxford: Oxford
University Press. doi:10.1093/oso/9780198732570.003.0002
Carey, Susan, 2009,
The Origin of Concepts
, Oxford:
Oxford University Press.
doi:10.1093/acprof:oso/9780195367638.001.0001
–––, 2014, “On Learning New Primitives in the Language of Thought: Reply to Rey”,
Mind and Language
, 29(2): 133–166.
doi:10.1111/mila.12045
Casati, Roberto and Achille C. Varzi, 1999,
Parts and Places:
The Structures of Spatial Representation
, Cambridge, MA: MIT
Press.
Chalmers, David J., 1990, “Syntactic Transformations on
Distributed Representations”,
Connection Science
,
2(1–2): 53–62. doi:10.1080/09540099008915662
–––, 1993, “Connectionism and
Compositionality: Why Fodor and Pylyshyn Were Wrong”,
Philosophical Psychology
, 6(3): 305–319.
doi:10.1080/09515089308573094
–––, 2012, “The Varieties of Computation:
A Reply”,
Journal of Cognitive Science
, 13(3):
211–248. doi:10.17791/jcs.2012.13.3.211
Chomsky, Noam, 1965,
Aspects of the Theory of Syntax
.
Cambridge, MA: MIT Press.
Churchland, Patricia S., 1986,
Neurophilosophy: Toward a
Unified Science of Mind-Brain
, Cambridge, MA: MIT Press.
Churchland, Patricia S. and Terrence J. Sejnowski, 1989,
“Neural Representation and Neural Computation”, in
Neural Connections, Neural Computation
, Lynn Nadel, Lynn A.
Cooper, Peter W. Culicover, and Robert M. Harnish, Cambridge, MA: MIT
Press.
Churchland, Paul M., 1990,
A Neurocomputational Perspective:
The Nature of Mind and the Structure of Science
, Cambridge, MA:
MIT Press.
Churchland, Paul M., and Patricia S. Churchland, 1990,
“Could a Machine Think?”,
Scientific American
,
262(1): 32–37. doi:10.1038/scientificamerican0190-32
Clark, Andy, 1991, “Systematicity, Structured
Representations and Cognitive Architecture: A Reply to Fodor and
Pylyshyn”, in Horgan and Tienson 1991: 198–218.
doi:10.1007/978-94-011-3524-5_9
–––, 2014,
Mindware: An Introduction to the
Philosophy of Cognitive Science
, second edition, Oxford: Oxford
University Press.
Cowie, Fiona, 1999,
What’s Within? Nativism
Reconsidered
, Oxford: Oxford University Press.
doi:10.1093/acprof:oso/9780195159783.001.0001
Cummins, Robert, 1989,
Meaning and Mental Representation
,
Cambridge, MA: MIT Press.
Dennett, Daniel C., 1977 [1981], “Critical Noticw: Review of
The Language of Thought by Jerry Fodor”,
Mind
, 86(342):
265–280. Reprinted as “A Cure for the Common Code”, in
Brainstorms: Philosophical Essays on Mind and Psychology
,
Cambridge, MA: MIT Press, 1981. doi:10.1093/mind/LXXXVI.342.265
–––, 1991, “Mother Nature Versus the
Walking Encyclopedia: A Western Drama”, in
Philosophy and
Connectionist Theory
, W. Ramsey, S. Stich, and D. Rumelhart,
Hillsdale, NJ: Lawrence Erlbaum Associates.
 [
available online
]
Devitt, Michael, 1995,
Coming to Our Senses: A Naturalistic
Program for Semantic Localism
, Cambridge: Cambridge University
Press. doi:10.1017/CBO9780511609190
Dretske, Fred, 1981,
Knowledge and the Flow of
Information
, Cambridge, MA: MIT Press.
–––, 1988.
Explaining Behavior
,
Cambridge, MA: MIT Press.
Egan, Frances, 1992, “Individualism, Computation, and
Perceptual Content”,
Mind
, 101(403): 443–459.
doi:10.1093/mind/101.403.443
Elman, Jeffrey L., 1989, “Structured Representations and
Connectionist Models”, in
Proceedings of the Eleventh Annual
Meeting of the Cognitive Science Society
, Mahwah: Laurence
Erlbaum Associates.
Field, Hartry, 2001,
Truth and the Absence of Fact
,
Oxford: Oxford University Press. doi:10.1093/0199242895.001.0001
Figdor, Carrie, 2009, “Semantic Externalism and the
Mechanics of Thought”,
Minds and Machines
, 19(1):
1–24. doi:10.1007/s11023-008-9114-6
Fodor, Jerry A., 1975,
The Language of Thought
, New York:
Thomas Y. Crowell.
–––, 1981,
Representations
, Cambridge,
MA: MIT Press.
–––, 1987,
Psychosemantics
, Cambridge,
MA: MIT Press.
–––, 1990,
A Theory of Content and Other
Essays
, Cambridge, MA: MIT Press.
–––, 1991, “Replies”, in
Meaning in Mind: Fodor and His Critics
, Barry M. Loewer and Georges Rey (eds.), Cambridge, MA: MIT Press.
–––, 1994,
The Elm and the Expert
,
Cambridge, MA: MIT Press.
–––, 1998,
Concepts: Where Cognitive Science
Went Wrong
, Oxford: Oxford University Press.
doi:10.1093/0198236360.001.0001
–––, 2001, “Doing without What’s within:
Fiona Cowie’s Critique of Nativism”,
Mind
, 110(437):
99–148. doi:10.1093/mind/110.437.99
–––, 2003,
Hume Variations
, Oxford: Oxford University Press.
doi:10.1093/acprof:oso/9780199287338.001.0001
–––, 2008,
LOT 2: The Language of Thought
Revisited
, Oxford: Oxford University Press.
doi:10.1093/acprof:oso/9780199548774.001.0001
–––, 2010, “Woof, Woof. Review of
The
Origin of Concepts
by Susan Carey”,
The Times Literary
Supplement
, October 8: pp. 7–8.
Fodor, Jerry and Brian P. McLaughlin, 1990, “Connectionism
and the Problem of Systematicity: Why Smolensky’s Solution Doesn’t
Work”,
Cognition
, 35(2): 183–204.
doi:10.1016/0010-0277(90)90014-B
Fodor, Jerry A. and Zenon W. Pylyshyn, 1981, “How Direct Is
Visual Perception?: Some Reflections on Gibson’s
‘Ecological Approach’”,
Cognition
, 9(2):
139–196. doi:10.1016/0010-0277(81)90009-3
–––, 1988, “Connectionism and Cognitive
Architecture: A Critical Analysis”,
Cognition
,
28(1–2): 3–71. doi:10.1016/0010-0277(88)90031-5
–––, 2015,
Minds Without Meanings
,
Cambridge, MA: MIT Press.
Frege, Gottlob, 1879 [1967],
Begriffsschrift, eine der
Arithmetischen Nachgebildete Formelsprache des Reinen Denkens
.
Translated as
Concept Script, a Formal Language of Pure Thought
Modeled upon that of Arithmetic
in
From Frege to Gödel:
A Source Book in Mathematical Logic, 1879–1931
, J. van
Heijenoort (ed.), S. Bauer-Mengelberg (trans.), Cambridge, MA: Harvard
University Press.
–––, 1892 [1997], “On
Sinn
and
Bedeutung
”. Reprinted in the
The Frege Reader
,
M. Beaney (ed.), M. Black (trans.), Malden, MA: Blackwell.
–––, 1918 [1997], “Thought”.
Reprinted in
The Frege Reader
, M. Beaney (ed.), P. Geach and
R. Stoothof (trans.), Malden, MA: Blackwell.
Gallistel, Charles R., 1990,
The Organization of
Learning
, Cambridge, MA: MIT Press.
Gallistel, Charles R. and Adam Philip King, 2009,
Memory and
the Computational Brain
, Malden, MA: Wiley- Blackwell.
Gallistel, C.R. and Louis D. Matzel, 2013, “The Neuroscience
of Learning: Beyond the Hebbian Synapse”,
Annual Review of
Psychology
, 64(1): 169–200.
doi:10.1146/annurev-psych-113011-143807
Gibson, James J., 1979,
The Ecological Approach to Visual
Perception
, Boston, MA: Houghton Mifflin.
Greenberg, Gabriel, 2013, “Beyond Resemblance”,
Philosophical Review
, 122(2): 215–287.
doi:10.1215/00318108-1963716
Greenberg, Mark, 2014, “Troubles for Content I”, in
Metasemantics: New Essays on the Foundations of Meaning
,
Alexis Burgess and Brett Sherman (eds.), Oxford: Oxford University
Press, 147–168. doi:10.1093/acprof:oso/9780199669592.003.0006
Harman, Gilbert, 1973,
Thought
, Princeton, NJ: Princeton
University Press.
Harnad, Stevan, 1994, “Computation Is Just Interpretable
Symbol Manipulation; Cognition Isn’t”,
Minds and
Machines
, 4(4): 379–390. doi:10.1007/BF00974165
Harnish, Robert M., 2002,
Minds, Brains, Computers: An
Historical Introduction to the Foundations of Cognitive Science
,
Malden, MA: Blackwell.
Haugeland, John, 1985,
Artificial Intelligence: The Very
Idea
, Cambridge, MA: MIT Press
Helmholtz, Hermann von, 1867 [1925],
Treatise on Physiological
Optics
(
Handbuch der physiologischen Optik
), James P.C.
Southall, Manasha, WI: George Banta Publishing Company.
Hinton, G. 1990. “Mapping Part-Whole Hierarchies into
Connectionist Networks”.
Artificial Intelligence
46:
pp. 47-75.
Horgan, Terence and John Tienson (eds.), 1991,
Connectionism
and the Philosophy of Mind
, (Studies in Cognitive Systems 9),
Dordrecht: Springer Netherlands. doi:10.1007/978-94-011-3524-5
–––, 1996,
Connectionism and the Philosophy

of Psychology
, Cambridge, MA: MIT Press.
Hume, David, 1739 [1978],
A Treatise on Human Nature
,

second edition, P. H. Nidditch (ed.). Oxford: Clarendon Press.
Jacobs, Lucia F and Randolf Menzel, 2014, “Navigation

Outside of the Box: What the Lab Can Learn from the Field and What the

Field Can Learn from the Lab”,
Movement Ecology
, 2(1):
3. doi:10.1186/2051-3933-2-3
Johnson, Kent, 2004, “On the Systematicity of Language and
Thought”:,
Journal of Philosophy
, 101(3): 111–139.
doi:10.5840/jphil2004101321
Johnson-Laird, Philip N., 2004, “The History of Mental
Models”, in
Psychology of Reasoning: Theoretical and
Historical Perspectives
, Ken Manktelow and Man Cheung Chung, New
York: Psychology Press.
Kant, Immanuel, 1781 [1998],
The Critique of Pure Reason
,
P. Guyer and A. Wood (eds), Cambridge: Cambridge University
Press.
Kaplan, David, 1989, “Demonstratives”, in
Themes
from Kaplan
, Joseph Almog, John Perry, and Howard Wettstein
(eds.), New York: Oxford University Press.
Kazez, Jean R., 1994, “Computationalism and the Causal Role
of Content”,
Philosophical Studies
, 75(3): 231–260.
doi:10.1007/BF00989583
King, Peter, 2005, “William of Ockham:
Summa
Logicae
”, in
Central Works of Philosophy: Ancient and
Medieval, volume 1: Ancient and Medieval Philosophy
, John Shand
(ed.), Montreal: McGill-Queen’s University Press,
242–270.
Knill, David C. and Whitman Richards (eds.), 1996,
Perception
as Bayesian Inference
, Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511984037
Knowles, Jonathan, 1998, “The Language of Thought and Natural Language Understanding”,
Analysis
, 58(4): 264–272. doi: 10.1093/analys/58.4.264
Kosslyn, Stephen, 1980,
Image and Mind
, Cambridge, MA: Harvard University Press.
Kulvicki, John, 2015, “Maps, Pictures, and
Predication”,
Ergo: An Open Access Journal of
Philosophy
, 2(7): 149–174.
Laurence, Stephen and Eric Margolis, 1997, “Regress
Arguments Against the Language of Thought”,
Analysis
,
57(1): 60–66.
Loar, Brian, 1982,
Mind and Meaning
, Cambridge: Cambridge
University Press.
Loewer, Barry, 1997, “A Guide to Naturalizing
Semantics”, in
A Companion to the Philosophy of
Language
, Bob Hale and Crispin Wright (eds.), Oxford:
Blackwell.
Lurz, Robert W. (ed.), 2009,
The Philosophy of Animal
Minds
, Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511819001
Mackintosh, Nicholas John, 2002, “Do Not Ask Whether They
Have a Cognitive Map, but How They Find Their Way about”,
Psicológica
, 23(1): 165–185.
 [
Mackintosh 2002 available online
]
Margolis, Eric, 1998, “How to Acquire a Concept”,
Mind & Language
, 13(3): 347–369.
doi:10.1111/1468-0017.00081
Margolis, Eric and Stephen Laurence, 2011, “Learning
Matters: The Role of Learning in Concept Acquisition”,
Mind
& Language
, 26(5): 507–539.
doi:10.1111/j.1468-0017.2011.01429.x
McDermott, Drew V., 2001,
Mind and Mechanism
, Cambridge,
MA: MIT Press.
McLaughlin, B. P. and T. A. Warfield, 1994, “The Allure of
Connectionism Reexamined”,
Synthese
, 101(3): 365–400.
doi:10.1007/BF01063895
Marcus, G., 2001,
The Algebraic Mind
, Cambridge: MIT
Press.
Millikan, Ruth Garrett, 1984,
Language, Thought, and Other
Biological Categories: New Foundations for Realism
, Cambridge,
MA: MIT Press.
–––, 1993,
White Queen Psychology and Other
Essays for Alice
, Cambridge, MA: MIT Press.
Neander, Karen, 2017,
A Mark of the Mental: In Defense of
Informational Teleosemantics
, Cambridge, MA: MIT Press.
Niklasson, Lars F. and Tim Gelder, 1994, “On Being
Systematically Connectionist”,
Mind & Language
,
9(3): 288–302. doi:10.1111/j.1468-0017.1994.tb00227.x
Normore, Calvin, 1990, “Ockham on Mental Language”, in
The Historical Foundations of Cognitive Science
, J. Smith
(ed.), Dordrecht: Kluwer.
–––, 2003, “Burge, Descartes, and
Us”, in
Reflections and Replies: Essays on the Philosophy of
Tyler Burge
, Martin Hahn and Bjørn Ramberg, Cambridge, MA:
MIT Press.
–––, 2009, “The End of Mental
Language”, in
Le Langage Mental du Moyen Âge à
l’Âge Classique
, J. Biard (ed.), Leuven:
Peeters.
O’Brien, Gerard and Jon Opie, 2006, “How Do Connectionist
Networks Compute?”,
Cognitive Processing
, 7(1):
30–41. doi:10.1007/s10339-005-0017-7
O’Keefe, John and Lynn Nadel, 1978,
The Hippocampus as a
Cognitive Map
, Oxford: Clarendon Press.
Ockham, William of, c. 1323 [1957],
Summa Logicae
,
Translated in his
Philosophical Writings, A Selection
,
Philotheus Boehner (ed. and trans.), London: Nelson, 1957.
Panaccio, Claude, 1999 [2017],
Mental Language: From Plato to
William of Ockham
(
Discours intérieur
), Joshua P.
Hochschild and Meredith K. Ziebart (trans.), New York: Fordham
University Press.
Papineau, David, 1987,
Reality and Representation
,
Oxford: Basil Blackwell.
Peacocke, Christopher, 1992,
A Study of Concepts
,
Cambridge, MA: MIT Press.
–––, 1994, “Content, Computation and
Externalism”,
Mind & Language
, 9(3): 303–335.
doi:10.1111/j.1468-0017.1994.tb00228.x
–––, 1999, “Computation as Involving
Content: A Response to Egan”,
Mind & Language
,
14(2): 195–202. doi:10.1111/1468-0017.00109
Perry, John, 1998, “Broadening the Mind”,
Philosophy and Phenomenological Research
, 58(1): 223–231.
doi:10.2307/2653644
Piccinini, Gualtiero, 2008, “Computation without
Representation”,
Philosophical Studies
, 137(2):
205–241. doi:10.1007/s11098-005-5385-4
Pinker, Steven, 2005, “So How Does the Mind Work?”,
Mind & Language
, 20(1): 1–24.
doi:10.1111/j.0268-1064.2005.00274.x
Pinker, Steven and Alan Prince, 1988, “On Language and
Connectionism: Analysis of a Parallel Distributed Processing Model of
Language Acquisition”,
Cognition
, 28(1–2): 73–193.
doi:10.1016/0010-0277(88)90032-7
Polger, Thomas W., 2004,
Natural Minds
, Cambridge, MA:
MIT Press.
Pollack, Jordan B., 1990, “Recursive Distributed
Representations”,
Artificial Intelligence
, 46(1–2):
77–105. doi:10.1016/0004-3702(90)90005-K
Prinz, Jesse, 2002,
Furnishing the Mind: Concepts and Their
Perceptual Basis
, Cambridge, MA: MIT Press.
–––, 2011, “Has Mentalese Earned Its Keep?
On Jerry Fodor’s LOT 2”,
Mind
, 120(478): 485–501.
doi:10.1093/mind/fzr025
Putnam, Hilary, 1967, “Psychophysical Predicates”, In
Art, Mind, and Religion: Proceedings of the 1965 Oberlin
Colloquium in Philosophy
, W.H. Capitan and D.D. Merrill (eds),
Pittsburgh, PA: University of Pittsburgh Press, 37–48.
–––, 1988,
Representation and Reality
,
Cambridge, MA: MIT Press.
Pylyshyn, Zenon W., 1984,
Computation and Cognition: Toward a
Foundation for Cognitive Science
, Cambridge, MA: MIT Press.
–––, 2003,
Seeing and Visualizing: It’s
Not What You Think
, Cambridge, MA: MIT Press.
Quine, W. V., 1951 [1980], “Two Dogmas of Empiricism”,
The Philosophical Review
, 60(1): 20–43. Reprinted in his
From a Logical Point of View
, second edition, Cambridge, MA:
Harvard University Press, 1980, 20–46. doi:10.2307/2181906
Ramsey, William M., 2007,
Representation Reconsidered
,
Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511597954
Rescorla, Michael, 2009a, “Chrysippus’ Dog as a Case Study
in Non-Linguistic Cognition”, in Lurz 2009: 52–71.
doi:10.1017/CBO9780511819001.004
–––, 2009b, “Cognitive Maps and the
Language of Thought”,
The British Journal for the Philosophy
of Science
, 60(2): 377–407. doi:10.1093/bjps/axp012
–––, 2009c, “Predication and Cartographic
Representation”,
Synthese
, 169(1): 175–200.
doi:10.1007/s11229-008-9343-5
–––, 2012a, “Are Computational Transitions
Sensitive to Semantics?”,
Australasian Journal of
Philosophy
, 90(4): 703–721.
doi:10.1080/00048402.2011.615333
–––, 2012b, “How to Integrate
Representation into Computational Modeling, and Why We Should”,
Journal of Cognitive Science
, 13(1): 1–37.
doi:10.17791/jcs.2012.13.1.1
–––, 2014a, “The Causal Relevance of
Content to Computation”,
Philosophy and Phenomenological
Research
, 88(1): 173–208.
doi:10.1111/j.1933-1592.2012.00619.x
–––, 2014b, “A Theory of Computational
Implementation”,
Synthese
, 191(6): 1277–1307.
doi:10.1007/s11229-013-0324-y
–––, 2015, “Bayesian Perceptual
Psychology”, in
The Oxford Handbook of Philosophy of
Perception
, Mohan Matthen (ed.), Oxford: Oxford University Press.
doi:10.1093/oxfordhb/9780199600472.013.010
–––, 2017a, “From Ockham to
Turing—and Back Again”, in
Philosophical Explorations
of the Legacy of Alan Turing
, Juliet Floyd and Alisa Bokulich
(eds.), (Boston Studies in the Philosophy and History of Science 324),
Cham: Springer International Publishing, 279–304.
doi:10.1007/978-3-319-53280-6_12
–––, 2017b, “Maps in the Head?”,
The Routledge Handbook of Philosophy of Animal Minds
, Kristin
Andrews and Jacob Beck (eds.), New York: Routledge.
–––, forthcoming, “Reifying
Representations”, in
What Are Mental Representations?
,
Tobias Schlicht, Krzysztof Doulega, and Joulia Smortchkova (eds.), Oxford:
Oxford University Press.
Rey, Georges, 2014, “Innate and Learned: Carey, Mad Dog
Nativism, and the Poverty of Stimuli and Analogies (Yet Again): Innate
and Learned”,
Mind & Language
, 29(2): 109–132.
doi:10.1111/mila.12044
Rumelhart, David and James L. McClelland, 1986, “PDP Models
and General Issues in Cognitive Science”, in Rumelhart, et al.
1986: 110–146.
Rumelhart, David E., James L. McClelland, and the PDP Research
Group, 1986,
Parallel Distributed Processing, volume 1:
Explorations in the Microstructure of Cognition: Foundations
,
Cambridge, MA: MIT Press.
Russell, Bertrand, 1918–1919 [1985], “The Philosophy
of Logical Atomism: Lectures 1-2”,
Monist
, 28(4):
495–527, doi:10.5840/monist19182843, 29(1): 32–63,
doi:10.5840/monist191929120, 29(2): 190–222,
doi:10.5840/monist19192922, 29(3): 345–380,
doi:10.5840/monist19192937. Reprinted in
The Philosophy of Logical
Atomism
, David F. Pears (ed.), La Salle, IL: Open Court.
Rupert, Robert D., 2008, “Frege’s Puzzle and Frege Cases:
Defending a Quasi-Syntactic Solution”,
Cognitive Systems
Research
, 9(1–2): 76–91.
doi:10.1016/j.cogsys.2007.07.003
Schiffer, Stephen, 1981, “Truth and the Theory of
Content”, in
Meaning and Understanding
, Herman Parret
and Jacques Bouveresse, Berlin: Walter de Gruyter, 204–222.
Schneider, Susan, 2005, “Direct Reference, Psychological
Explanation, and Frege Cases”,
Mind & Language
,
20(4): 423–447. doi:10.1111/j.0268-1064.2005.00294.x
–––, 2011,
The Language of Thought: A New
Philosophical Direction
, Cambridge, MA: MIT Press.
Sellars, Wilfrid, 1975, “The Structure of Knowledge”,
in
Action, Knowledge and Reality: Studies in Honor of Wilfrid
Sellars
, Hector-Neri Castañeda (ed.), Indianapolis, IN:
Bobbs-Merrill, 295–347.
Shagrir, Oron, forthcoming, “In Defense of the Semantic View
of Computation”,
Synthese
, First online: 11 October
2018. doi:10.1007/s11229-018-01921-z
Shea, Nicholas, 2016, “Representational Development Need Not
Be Explicable-By-Content”, in
Fundamental Issues of
Artificial Intelligence
, Vincent C. Müller (ed.), Cham:
Springer International Publishing, 223–240.
doi:10.1007/978-3-319-26485-1_14
Sloman, Aaron, 1978,
The Computer Revolution in Philosophy:
Philosophy, Science and Models of the Mind
, Hassocks: The
Harvester Press.
Smolensky, Paul, 1990, “Tensor Product Variable Binding and
the Representation of Symbolic Structures in Connectionist
Systems”,
Artificial Intelligence
, 46(1–2):
159–216. doi:10.1016/0004-3702(90)90007-M
–––, 1991, “Connectionism, Constituency,
and the Language of Thought”, in
Meaning in Mind: Fodor and
His Critics
, Barry M. Loewer and Georges Rey (eds), Cambridge,
MA: Blackwell.
–––, 1995, “Constituent Structure and
Explanation in an Integrated Connectionist/Symbolic Cognitive
Architecture”, in
Connectionism: Debates on Psychological
Explanation
, Cynthia Macdonald and Graham Macdonald (eds),
Oxford: Basil Blackwell.
Stalnaker, Robert C., 1984,
Inquiry
, Cambridge, MA: MIT
Press.
Stich, Stephen P., 1983,
From Folk Psychology to Cognitive
Science
, Cambridge, MA: MIT Press.
Tarski, Alfred, 1933 [1983], “Pojęcie prawdy w językach
nauk dedukcyjnych”, Warsaw: Nakładem Towarzystwa Naukowego
Warszawskiego. Translated into German (1935) by L. Blaustein as
“Der Wahrheitsbegriff in den formalisierten Sprachen”,
Studia Philosophica
, 1: 261–405. Translated into
English (1983) as “The Concept of Truth in Formalized
Languages”, in
Logic, Semantics, Metamathematics: Papers
from 1923 to 1938
, second edition, J.H. Woodger (trans.), John
Corcoran (ed.), Indianapolis, IN: Hackett.
Tolman, Edward C., 1948, “Cognitive Maps in Rats and
Men.”,
Psychological Review
, 55(4): 189–208.
doi:10.1037/h0061626
Touretzky, David S., 1990, “BoltzCONS: Dynamic Symbol
Structures in a Connectionist Network”,
Artificial
Intelligence
, 46(1–2): 5–46.
doi:10.1016/0004-3702(90)90003-I
Turing, Alan M., 1936, “On Computable Numbers, with an
Application to the Entscheidungsproblem”,
Proceedings of the
London Mathematical Society
, s2-42(1): 230–265.
doi:10.1112/plms/s2-42.1.230
van Gelder, Timothy, 1991, “Classical Questions, Radical
Answers: Connectionism and the Structure of Mental
Representations”. In Horgan and Tienson 1991: 355–381,
doi:10.1007/978-94-011-3524-5_16
Wakefield, Jerome C., 2002, “Broad versus Narrow Content in
the Explanation of Action: Fodor on Frege Cases”,
Philosophical Psychology
, 15(2): 119–133.
doi:10.1080/09515080220127099
Weiner, Jan, Sara Shettleworth, Verner P. Bingman, Ken Cheng,
Susan Healy, Lucia F. Jacobs, Kathryn J. Jeffery, Hanspeter A. Mallot,
Randolf Menzel, and Nora S. Newcombe, 2011, “Animal Navigation:
A Synthesis”, in
Animal Thinking
, Randolf Menzel and
Julia Fischer (eds), Cambridge, MA: MIT Press.
Wittgenstein, Ludwig, 1921 [1922],
Logisch-Philosophische
Abhandlung
, in W. Ostwald (ed.),
Annalen der
Naturphilosophie
, 14. Translated as
Tractatus
Logico-Philosophicus
, C.K. Ogden (trans.), London: Kegan Paul,
1922.
–––, 1953,
Philosophical
Investigations
, G.E.M. Anscombe (trans.), Oxford: Blackwell.
Academic Tools
How to cite this entry
.
Preview the PDF version of this entry
at the
Friends of the SEP Society
.
Look up topics and thinkers related to this entry
at the Internet Philosophy Ontology Project (InPhO).
Enhanced bibliography for this entry
at
PhilPapers
, with links to its database.
Other Internet Resources
Aydede, Murat, “The Language of Thought Hypothesis,”
Stanford Encyclopedia of  Philosophy
(Spring 2019 Edition),
 Edward N. Zalta (ed.), URL =
 <
https://plato.stanford.edu/archives/spr2019/entries/language-thought/
>.
  [This was the previous entry on the langugage of thought 
hypothesis in the
Stanford Encyclopedia of Philosophy
— 
see the
version history
.]
Bibliography on the language of thought
,
 in PhilPapers.org.
Bibliography on the philosophy of artificial intelligence
,
 curated by Eric Dietrich, in PhilPapers.org.
Related Entries
artificial intelligence
|
belief
|
Church-Turing Thesis
|
cognitive science
|
computation: in physical systems
|
concepts
|
connectionism
|
consciousness: representational theories of
|
folk psychology: as a theory
|
functionalism
|
intentionality
|
mental content: causal theories of
|
mental imagery
|
mental representation
|
mind: computational theory of
|
naturalism
|
physicalism
|
propositional attitude reports
|
qualia
|
reasoning: automated
|
Turing, Alan
|
Turing machines
Acknowledgments
I owe a profound debt to the Murat Aydede, author of the
previous entry
on the same topic. His exposition hugely
influenced my work on the entry, figuring indispensably as a
springboard, a reference, and a standard of excellence. Some of my
formulations in the introduction and in sections 1.1, 2, 3, 4.3, 5,
6.1, and 7 closely track formulations from the previous entry. Section
5’s discussion of connectionism is directly based on the
previous entry’s treatment. I also thank Calvin Normore, Melanie
Schoenberg, and the Stanford Encyclopedia editors for helpful
comments.