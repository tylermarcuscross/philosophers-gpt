Higher-Order Evidence
First published Tue Sep 6, 2022
Higher-order evidence is evidence which bears on a believer’s
rational capacities, epistemic performance, or evidential situation.
Many epistemologists hold that this kind of evidence can rationally
affect our “first-order” beliefs as well: that is, those
beliefs about the world that we form using the relevant rational
capacities, or in the relevant evidential situation. It is easiest to
get a grip on this phenomenon by looking at examples. Here is a
paradigmatic case (paraphrased from Schoenfield 2018: 690, based on
similar cases put forth by Adam Elga [2013 and 2008—see Other
Internet Resources], Lasonen-Aarnio [2014], Christensen [2010a], and
others):
Hypoxia
: Aisha is out flying her small, unpressurized
airplane, wondering whether she has enough fuel to make it to Hawaii.
She looks at the gauges, dials, and maps, and obtains some evidence,
E
, which she knows strongly supports (say to degree .99)
either the proposition that she has enough gas (
G
) or that
she does not (~
G
). Thinking it over and performing the
necessary calculations, Aisha concludes
G
; in fact, this is
what
E
supports. But then she checks her altitude and notices
that she’s at great risk for hypoxia, a condition which impairs
one’s reasoning while leaving the reasoner feeling perfectly
cogent and clear-headed. Aisha knows that at this altitude, pilots
performing the kinds of calculations she just did only reach the
correct conclusion 50% of the time.
Aisha’s “first-order” evidence is her evidence
bearing directly on her beliefs about
G
: that is, the
evidence she receives by consulting her dials, gauges, and maps. Her
“higher-order” evidence is her evidence suggesting that
she’s at risk for hypoxia. Higher-order evidence bears directly
on Aisha’s epistemic situation in various ways, but does not
bear directly on
G
. The primary question that this case
raises, for the higher-order evidence literature, is: does
Aisha’s higher-order evidence
also
rationally affect
her beliefs about
G
? (That is, does it bear on
G
in
some
indirect
way?)
Some say yes. It would be reckless for Aisha to head out over the
Pacific after realizing that there is such a serious chance that
she’s rationally impaired, which surely shows (some argue) that
she should not be very confident of
G
. Others say no. As the
case stipulates, Aisha’s first-order evidence strongly supports
G
; whether Aisha is hypoxic has absolutely nothing to do with
whether she has enough gas to make it to Hawaii. (Why would her
total
evidence prompt her to revise confidence in
G
,
when the new evidence she has gained is
irrelevant
to
G
?) Answering “yes” raises questions about
exactly how and why this level-interaction works. Answering
“no” raises questions about how to explain the intuitive
unreasonableness of Aisha’s maintaining high confidence in
G
.
Higher-order evidence thus raises a puzzle. Two apparent features of
Hypoxia are in tension with one another: it seems that Aisha is
required to reduce her confidence in
G
, and yet it also seems
that her evidence still strongly supports
G
. Some see this as
a conflict between the belief state that seems intuitively rational
(reduced confidence or suspension of judgment about
G
) and
the belief state that seems strongly supported by her evidence (high
confidence in
G
). Others see it as a conflict within her
beliefs. If Aisha maintains high confidence in
G
, but also
takes seriously the possibility that she could be hypoxic, she will be
in a state of
epistemic akrasia
: she will have a belief state
(her high confidence in
G
) which she also judges likely to be
irrational, or unsupported by her evidence.
In what follows we will see these issues crop up in several different
forms. This article will be organized around a few core questions
about higher-order evidence, which will give us different ways to
approach these central puzzles.
Is higher-order evidence different from ordinary evidence? If so,
how?
Why might we
deny
that higher-order evidence has a
rational effect on first-order beliefs? And if we take that approach,
which problems and questions remain?
If higher-order evidence rationally affects first-order beliefs,
how does this work? What sort of principle might govern that
interaction?
Is there a way to split the difference, embracing both sides of
the puzzle? For example, might there be different, incompatible
epistemic norms at work in cases like Hypoxia, one telling Aisha to
believe
G
and another telling her to suspend judgment?
A few quick notes before moving on. Following most of the literature,
the term “higher-order evidence” is used here, although it
would be more accurate to speak of higher-order
evidential
effects
or higher-order
evidential import
(since one
piece of evidence may be relevant to a person’s beliefs about
many different subject matters, in a number of different ways). And
like most of the literature, the focus is on higher-order
defeat
, rather than higher-order
confirmation
, even
though these possibilities go hand in hand. Some of the issues
discussed in this article are discussed in more detail in the entry on
epistemic self-doubt
.
 Finally, although the higher-order evidence debate has its roots in
the literature on disagreement, this entry focuses on the simpler,
single-person case introduced above and mostly omits disagreement.
1. What is Distinctive About Higher-Order Evidence?
2. Denying the Import of Higher-Order Evidence
2.1 Structural problems for higher-order evidence
2.2 Higher-order defeat and self-undermining
2.3 Consequences of denying the import of higher-order evidence
3. Accommodating the Import of Higher-Order Evidence
3.1 Independence principles
3.2 Level-bridging principles
3.2.1 Rationality-focused principles
3.2.2 Reliability-focused principles
4. Dilemmas and Two-Norm Views
4.1 Dilemmas
4.2 Ideal vs. non-ideal modes of evaluation
4.3 Best plans to follow vs. best plans to make
4.4 Reasons vs. rationality
4.5 Evidence vs. dispositions
4.5 Propositional justification vs. doxastic justification
5. Conclusion
Bibliography
Academic Tools
Other Internet Resources
Related Entries
1. What is Distinctive About Higher-Order Evidence?
Higher-order evidence was introduced above by example. We can come
across higher-order evidence in a variety of ways: brain-altering
conditions like hypoxia, or drugs, but also more mundane sources like
bias and fatigue. Many epistemologists have discussed
disagreement
as a source of higher-order evidence: if an
equally well-informed and thoughtful person looks at the evidence and
draws a different conclusion from yours, this arguably gives you
reason to worry that you’ve made a rational error (see, e.g.,
Kelly 2005, Feldman 2006, Christensen 2007b, Elga 2008 and much
following literature). And some have argued that “irrelevant
influences” on belief, such as one’s religious upbringing,
can be a source of higher-order evidence as well: the realization that
you would have had different religious beliefs if you had been raised
in a different community, for example, might give you reason to doubt
the rationality of those beliefs (see Elga 2008—see Other
Internet Resources, White 2010, and Vavova 2018). It is hard to find
an uncontroversial definition of higher-order evidence, as different
authors understand it in different ways. But even without a general
definition, we can look at cases like Hypoxia to see how higher-order
evidence is importantly distinctive. (This section assumes that
higher-order evidence has some first-order rational import, as this is
where the distinctive features arise.)
At first glance, one might think that Hypoxia is simply a case of
undercutting defeat, and Aisha’s justification for believing
G
is defeated just as one’s perceptual beliefs might be
defeated by, for instance, evidence of tricky lighting. But as Feldman
(2005) argues, higher-order defeat and undercutting defeat are
importantly different. Compare Hypoxia to a case in which you find out
that a certain wall, which appears to be red, is illuminated by a red
light. In the latter case, you might recognize that your experience as
of a red wall
generally
supports the belief that the wall is
red, and also recognize that you responded appropriately to that
evidence before finding out about the tricky lighting. But in Hypoxia
it seems that Aisha is not in a position to recognize either of these
things. If she takes her higher-order evidence seriously, she will
come to doubt whether her first-order evidence ever supported
G
in the first place. And she will also doubt whether she
evaluated it correctly. (See Coates 2012 and Christensen 2010a for
similar discussion.) So higher-order defeat is not simply undercutting
defeat.
Joshua DiPaolo (2018) argues that higher-order defeat is also
distinctive in that it is “object-independent”. For
instance, learning that there is a red light on the wall will undercut
your belief that the wall is red, but will have no effect if you
believe that the wall is not red. Higher-order evidence, on the other
hand, typically does not discriminate based on the contents of the
beliefs it targets. Extending DiPaolo’s argument to our case
above, Aisha’s higher-order evidence targeted her belief about
G
, but could have easily targeted a belief with different
contents if she had been reasoning about a different matter while
flying at that altitude.
Christensen (2010a) argues that higher-order evidence is distinctive
because its import is agent-relative. While Aisha's information about
hypoxia gives
Aisha
reason to doubt her beliefs about whether
she will make it to Hawaii, it would have no such effect on another
person looking at the same charts and dials from the safety of the
ground. However, merely treating higher-order evidence as indexical
does not seem to make it less puzzling; see Schoenfield 2018 for
further discussion.
Perhaps the most puzzling feature of higher-order evidence is that it
seems to obligate agents to ignore or set aside parts of their total
evidence. (See Christensen 2010a for an early defense of this idea;
others, including Elga (2007), also defend it in the context of
disagreement.) Even as Aisha comes to doubt her conclusion about
G
, her first-order evidence and reasoning is still plainly
before her. But she cannot rationally appeal to it in forming her
beliefs. If she could, Aisha might argue:
although I’m at an altitude that renders me susceptible to
hypoxia, I must be immune. After all, my first-order evidence in fact
supports the conclusion that I have enough fuel to make it to Hawaii,
and that’s the very same conclusion I reached!
To a supporter of higher-order defeat, such reasoning looks irrational
and dogmatic. (Furthermore, if she could reason this way, Aisha could
arguably keep her high confidence in
G
—which his just
to say, her higher-order evidence wouldn’t have a rational
effect after all.) In order to rule out the rationality of such
reasoning, while acknowledging that Aisha’s first-order evidence
has not vanished, some epistemologists say that Aisha must set aside
the targeted first-order evidence and reasoning once her higher-order
evidence comes in. This thought is often called
“Independence”.
Section 3.1
discusses it in more detail.
Finally, related to this last observation, Horowitz (2019) and
Schoenfield (2015b, 2018) both argue that higher-order evidence is
predictably misleading: if it has first-order effects, we can predict
a priori that it will lead ideally rational thinkers
away
from the truth. We expect first-order evidence to generally make us
more accurate, provided that we respond to it rationally. And we
expect higher-order evidence to lead us away from what our first-order
evidence supports. So when we accommodate all of that evidence
rationally, we’ll tend to end up with a
less
accurate
belief state than we would if we had ignored the higher-order
evidence.
In all of these ways, higher-order evidence presents distinctive
problems for epistemology. (Though distinctive does not necessarily
mean rare: Hedden and Dorst [forthcoming] argue that almost all
evidence has higher-order import.) As we will see in the next section,
the difficulties presented by higher-order evidence have prompted some
epistemologists to deny that it has first-order import altogether.
2. Denying the Import of Higher-Order Evidence
Higher-order evidence raises special problems. One very general
problem is that it is hard to see how we could make sense of
higher-order evidence within a consistent, total picture of
rationality. Maria Lasonen-Aarnio (2014) brings out this point
especially forcefully and comprehensively, so this next section will
largely follow her presentation. (See also Schechter 2013 for
discussion of many similar concerns.) We will then look at a slightly
different objection to theories that accommodate higher-order
evidence: the charge that they are
self-defeating
. The
section will conclude with some options for theories according to
which higher-order evidence does
not
rationally affect
first-order beliefs.
2.1 Structural problems for higher-order evidence
Lasonen-Aarnio begins with the observation that respecting
higher-order evidence can compel a rational agent to violate genuine
epistemic rules. (This is very similar to the idea presented in the
introduction: that after Aisha receives her higher-order evidence, her
total evidence
still supports
G
.) Since this thought
is common to much of the higher-order evidence literature, it will be
worth spelling out in detail the reasoning behind it. One simple way
to make the thought plausible is to focus on a case of
entailment—so, let’s suppose Aisha’s first-order
evidence entails
G
. Then we can argue as follows:
P1.
Aisha’s first-order evidence entails
G
.
P2.
After Aisha receives her higher-order evidence, her total evidence
entails
G
.
P3.
It is a rational requirement to believe what our evidence
entails.
P4.
After Aisha receives her higher-order evidence, she is rationally
required to believe
G
.
P5.
After Aisha receives her higher-order evidence, she is rationally
required to suspend judgment on
G
.
C.
After receiving her higher-order evidence, Aisha is rationally
required to violate a rational requirement.
P1
is built into (this version of) our case.
P2
follows from
P1
,
 assuming that Aisha’s body of total evidence grows
monotonically over the course of the story.
P3
is plausible, and would nicely explain our verdict at the beginning
of the story: that, before she receives her higher-order evidence,
Aisha should believe
G
.
P4
follows from
P2
and
P3
.
 And
P5
is the intuitive verdict that many epistemologists share about cases
like Hypoxia. (Though see Henderson forthcoming, Staffel forthcoming,
and Steglich-Peterson 2019 for alternative views on which higher-order
evidence does not require suspension of judgment, but still has a
rational effect.) So if
P5
is right, this means that Aisha is required to disobey an epistemic
rule which applies to her current situation: she must suspend judgment
in a proposition that is entailed by her evidence, rather than believe
it.
Though focusing on entailment makes the argument above particularly
clear, it’s not essential. The same problem can, plausibly,
arise with non-entailing evidence as well. Lasonen-Aarnio focuses on a
perceptual case, and Christensen (2010a) argues that cases involving
inductive reasoning can lead to the same conclusion. (Both argue for
versions of
P2
in non-entailing contexts: they argue, respectively, that the
perceptual or inductive support remains even in the face of
higher-order undermining.)
How can we make sense of Aisha’s situation, in light of this
apparent conflict? Lasonen-Aarnio (2014) surveys several possible ways
to make room for the conflict within our overall epistemological
theory. One strategy we might try is to say that epistemic rules have
built-in exceptions for higher-order defeat. That doesn’t solve
the problem, Lasonen-Aarnio argues, but just pushes it back: if any
rule can be defeated, then so can rules with built-in exceptions. A
second strategy is to say that epistemic rules are hierarchical, with
some taking precedence over others; but again, the higher-level rules
must themselves be defeasible, leading to an infinite hierarchy. The
rules must also be ordered by a “meta-rule” which
determines which rule governs one’s current situation; what if
the meta-rule itself is defeasible? It seems that we are either left
with an infinite regress, or that we must accept a stopping
point—a rule which itself cannot be rationally defeated. Another
strategy is to posit an “Über-rule” which specifies
the rational response for each unique situation. This fails,
Lasonen-Aarnio argues, because it is too complex for us to grasp, too
different from the epistemic rules we ordinarily take ourselves to
follow, and endorsing it leaves us with an unsatisfying kind of
particularism about rationality. This proposal also runs into the same
problem we saw above. If the Über-Rule is always correct, we must
say that it cannot be rationally defeated. If we are willing to accept
indefeasible rules, why should we think that higher-order defeat
happens at all? (Why not just accept the rules governing Aisha’s
belief in
G
as themselves indefeasible by higher-order
doubt?) Lasonen-Aarnio concludes that in light of these problems, we
should reject higher-order defeat. (See Bradley 2019 for response to
Lasonen-Aarnio, and a defense of the Über-rule.)
2.2 Higher-order defeat and self-undermining
Another route to doubt about higher-order defeat is what’s
sometimes called the “self-undermining” objection. As
several epistemologists have pointed out, there are strange
consequences for anyone who
believes a theory
on which
higher-order evidence has first-order effects: it seems that this
belief is liable to undercut its own justification. Let’s
consider a candidate principle on which higher-order evidence can
defeat first-order beliefs; call it “HOD” (for
“higher-order defeat”). Now consider a person, Sam, who
believes that HOD is a true principle of rationality. What happens if
Sam’s belief in HOD is
itself
a target of higher-order
defeat? This might happen through peer disagreement (which is the
focus of most of the literature on this objection) or other means.
Afterwards, according to HOD, Sam cannot rationally believe HOD.
This might already seem like a problem to some: a true theory of
rationality should not (arguably) undermine our justification for
believing it. And taking it a step further, it can look like the
theory is either paradoxical or incoherent. If HOD can call for its
own rejection (in the cases where belief in HOD is itself undermined),
how can it give us coherent directions? Suppose Sam’s
higher-order evidence targets two beliefs simultaneously: his belief
about some proposition
P
, and his belief about HOD itself.
HOD tells him to reduce confidence in
P
. But since HOD is
undermined as well, it is arguably
also
telling him not to
reduce confidence (or to reduce confidence to a lesser degree). These
recommendations are incompatible, yielding the worry that HOD is
internally inconsistent. On the other hand, the worry about paradox
arises when we consider what happens when Sam begins to revise his
beliefs. Once HOD is undermined, suppose Sam no longer believes it,
and goes back to revise the beliefs that he formed on its basis. But
his last application of HOD was what led to his doubting HOD—so
if he “undoes” that step, he’ll believe HOD again!
But then his belief in HOD will be undermined by his higher-order
evidence, and the loop will start over. (For further discussion see
Elga 2010, Weatherson 2013, Christensen 2013 and 2021a, and Bradley
2019. See Roush 2009 for discussion of a related phenomenon.)
Advocates of higher-order defeat have offered a few different
responses to this class of objections. One option, proposed by Elga
(2010), is to say that principles like HOD are exempt from
undermining. (Elga’s view draws on an argument found in Lewis
1971 and H. Field 2000, for the conclusion that our most fundamental
belief-forming methods must be self-recommending.) Another option is
to say that our attitude towards principles like HOD is not one of
belief
, but some other type of attitude. (See Goldberg 2013,
Barnett 2019, and Fleisher 2021 for examples of this approach.)
Finally, one might simply deny that Sam is obligated to enter the
paradoxical loop once his belief in HOD is undermined. Christensen
(2013, 2021a, 2021b) argues along these lines. The loop gets started
if we think that once Sam rationally doubts HOD, he is rationally
required to stop adopting the beliefs that HOD recommends, and instead
adopt whichever beliefs he now regards as most rational. But this
thought is not part of HOD, and we are free to deny it—we can
maintain that HOD is true, and rationally binding even for those who
rationally doubt it. (This is different from saying that HOD must be
self-recommending; on Elga’s proposal, one should
believe
that one should always follow HOD, and also always
follow it. On Christensen’s, one should always follow HOD,
regardless of what one should believe one should do.) This response
would bypass the paradox. But it comes at what some defenders of HOD
see as a cost: it means we must accept some instances of rational
epistemic akrasia. If Sam revises his belief in HOD, but continues to
follow it, as this suggestion would have him do, he will end up with a
belief state whose rationality he doubts.
In the face of these difficulties, some epistemologists conclude that
higher-order defeat is simply not a genuine phenomenon. These views
are discussed next. In sections 3 and 4, we will come back to the
options for accommodating higher-order defeat.
2.3 Consequences of denying the import of higher-order evidence
If we hold that higher-order evidence does not have first-order
rational effects, what should we say about cases like Hypoxia? There
are two mainstream options defended in the literature.
“Level-Splitters” suggest that Aisha’s first-order
belief (about
G
) should follow her first-order evidence, so
she should be highly confident of
G
. But her higher-order
belief (about her own rationality, or about what her first-order
evidence supports) should follow her higher-order evidence.
“Steadfasters” argue that Aisha should just dismiss the
higher-order evidence altogether—believe
G
, and believe
that her evidence supports it. (See Alexander 2013, however, for an
argument that
no
response is justified in Aisha’s case,
and Leonard 2020 for an argument that it is indeterminate what Aisha
should believe.)
Defenders of the Level-Splitting view include Lasonen-Aarnio (2014),
Coates (2012), Weatherson (ms–see Other Internet Resources),
Williamson (2014), and Wedgwood (2012). The obvious advantage is that
this view straightforwardly takes all of Aisha’s evidence into
account: her evidence about
G
affects her beliefs about
G
, and her evidence about her epistemic situation affects her
beliefs about her epistemic situation. An obvious upshot of this view
is that epistemic akrasia can be rational: in particular, in Hypoxia,
Aisha should end up believing
G
, while also believing that
her evidence likely doesn’t support
G
.
But admitting rational epistemic akrasia in this case incurs a large
intuitive cost. As Horowitz (2014) argues, if epistemic akrasia is
rational in cases like Aisha’s, then so is bootstrapping: if
Aisha finds herself in this situation a number of times in a row, or
regarding a number of different beliefs, she can use her first-order
beliefs to establish a fantastic track record of success, ultimately
dismissing the possibility that she was ever rationally impaired. This
reasoning looks absurd—because, presumably, there is a tension
between strongly holding a belief while also believing one’s
evidence doesn’t support it. So why allow this tension in even
one case? Horowitz also illustrates other examples of irrational
reasoning and action that seem warranted by epistemic akrasia; see
also Brown 2018, Littlejohn 2018, and Silva 2018 for further
discussion of intuitively irrational reasoning and action licensed by
epistemic akrasia in these cases, as well as Feldman 2005 for an
earlier rejection of epistemic akrasia in similar circumstances.
The second possibility is to say that higher-order evidence simply has
no effect on first-order
or
higher-order beliefs. (Following
the peer disagreement literature, we could call this a
“steadfast” view; Smithies [2019] calls it “upward
push”.) Kelly (2005) seems to tentatively support this view in
the context of peer disagreement (though his later work defends a more
moderate “total evidence” view; see, for example, Kelly
2010). Titelbaum (2015) argues for the nearby position that a rational
agent can never be mistaken about the rational requirements that apply
to her situation (although she might be rationally uncertain as to
which situation she is in); he writes that “mistakes about
rationality are mistakes
of
rationality”.
Titelbaum’s argument for this rests on the assumption that
epistemic akrasia is irrational. (See also Titelbaum 2019. See C.
Field 2019 for criticism of this position.) Tal (2021) also defends
the steadfast view, in part appealing to the irrationality of
epistemic akrasia.
Smithies (2019; see especially ch. 10) gives an extended defense of a
steadfast view. He argues that ideal rationality requires rational
omniscience—that is, omniscience both about what the rational
requirements are and which requirements apply to one at all times.
This means that misleading higher-order evidence is, in an important
sense, impossible to come by. While non-ideal agents like Aisha may be
mistaken about what evidence they have or what it supports, these
mistakes are themselves departures from ideal rationality. (Smithies
also supplements this with a view about non-ideal rationality, which
is discussed below.)
One of the primary objections to the steadfast view is that ignoring
higher-order evidence appears to be blatantly dogmatic: it is hard to
see how it could be rational for Aisha to continue believing
G
with no reduction in confidence, after hearing that her
altitude puts her at risk of hypoxia. (Though Tom Kelly has argued
[see Kelly 2013, for example] that dogmatism in such cases is not
inherently irrational.) Additionally, acting on her belief that
G
—say, setting off for Hawaii with a plane full of
passengers—would be shockingly irresponsible, suggesting that
something is wrong with the belief that
G
. (See Christensen
2010a, for example.)
Cases like Hypoxia therefore leave us with three choices. If we say
that Aisha should maintain confidence in
G
, but reduce
confidence that it’s rational for her to belief
G
, we
must explain why epistemic akrasia can be rational. If we say she
should maintain confidence in
G
and dismiss the possibility
of hypoxia, we must explain why dogmatism can be rational. And if we
say she should reduce confidence in
G
, we must explain how
this type of defeat works, and how it fits into our broader theory of
rationality. Let us now return to that last question.
3. Accommodating the Import of Higher-Order Evidence
In this section we will look more closely at the possibilities for
accommodating higher-order defeat. Even setting aside questions about
how such defeat fits into the rest of our epistemological theory,
there are several more local questions about how higher-order defeat
works. What sort of principle could explain the intuitive verdicts in
cases like Hypoxia? Which evidence and reasoning can agents like Aisha
rely on in forming their beliefs, and under what circumstances? What
determines which attitude, exactly, Aisha should have towards
G
after taking both her first-order and her higher-order
evidence into account?
There are a couple of different ideas that epistemologists sympathetic
to higher-order evidence typically aim to capture. One is the thought
that higher-order evidence requires us to “bracket” or set
aside some of our first-order evidence and reasoning. This is often
called “Independence”. The second is the thought that, in
accommodating one’s higher-order evidence, one should somehow
adjust one’s first-order belief to
match
what one has
learned from the higher-order evidence: either to match what one
believes or has reason to believe would be
rational
, to match
one’s expected level of
reliability
, or some variation
on one of these. Principles dictating how one should calibrate
one’s first-order beliefs in light of higher-order evidence are
often called “level-bridging principles” or
“calibration principles”.
Many specific proposals for how to accommodate higher-order evidence
(often focusing on the case of disagreement, which we can think of as
a variety of higher-order defeat) combine both of these ideas. For
example, here is Elga’s (2007: 490) formulation of the
“Equal Weight View” of disagreement:
Equal weight view
Upon finding out that an advisor
disagrees, your probability that you are right should equal your prior
conditional probability that you would be right. Prior to what? Prior
to your thinking through the disputed issue, and finding out what the
advisor thinks of it. Conditional on what? On whatever you have
learned about the circumstances of the disagreement.
Calibrating your confidence to the probability that “you would
be right” is a way of matching your confidence in P to your
expected reliability about P, so the Equal Weight View is in part a
level-bridging principle. And the “prior” and
“conditional” clauses commit Elga to an Independence
principle as well: he is saying that we must
set aside
the
specific first-order evidence and reasoning about the relevant matter,
and consider only the surrounding “circumstances”. In the
very next paragraph Elga clarifies that “whatever you have
learned about the circumstances of the disagreement” must be
independent from the reasoning leading to the disagreement itself.
Instead, it should include facts about the situation that would affect
your or your peer’s reliability, such as how much coffee you
have each had, how absurd you find the other person’s answer,
and whether the reasoning invites a type of mistake that you or your
friend is especially likely to make.
The next two subsections discuss Independence and level-bridging
separately. But many authors who endorse one of these (in one form or
another) also endorse the other.
3.1 Independence principles
Independence principles say, roughly, that higher-order evidence calls
for us to set aside some of our evidence, and not rely on it in our
reasoning. This thought is required to rule out the kind of dogmatic
response to higher-order evidence discussed above: if we did
not
have to set aside our first-order evidence and reasoning,
it would be rational to rely on it to dismiss any higher-order doubt.
Independence raises a number of questions. How can it be rational to
ignore evidence? What, exactly, do we have to ignore? And does
Independence open the door to skepticism?
Some balk at Independence because it simply seems irrational to ignore
evidence (see Kelly 2005, 2010, for example). Kelly (2010) presents
the objection roughly like this: if higher-order evidence requires us
to ignore our first-order evidence, he asks, doesn’t that make
rationality far too easy to come by? It appears that someone could
botch their first-order reasoning completely, adjust their confidence
in the face of higher-order evidence, and (since they are now
obligated to ignore the first-order considerations) have all of their
earlier rational mistakes forgiven. Christensen (2011) disagrees,
arguing that while someone in this situation may have responded to
part
of her evidence correctly, we can still distinguish
between her final belief state and the belief state of someone who
didn’t make the initial mistake: someone who did every step
properly is more rational than someone who did not. (See Sliwa &
Horowitz 2015 for further discussion; see Schoenfield 2015a for an
objection.)
If Independence is right, there are remaining questions about exactly
what needs to be set aside—evidence? Reasoning? Something
else?—and under what circumstances. As several authors have
brought out, it is not quite right to simply say that our assessment
must be independent of our first-order reasoning; often,
facts
about
that reasoning, sometimes involving very specific aspects
of our evidence, are highly relevant and it would be irrational to
ignore them. (See Arsenault & Irving 2012, Kelly 2013, and Lord
2014 for versions of this objection; see Christensen 2018 and 2019 for
replies.) It is also not quite right to say that our assessment must
be independent of our first-order evidence; often, a single piece of
evidence has several different effects, some of which should be set
aside and others of which should be taken into account. See
Christensen 2019 for detailed discussion of these complexities and
others.
A different sort of question concerns how
much
evidence,
information, or background beliefs we must set aside, and whether
there are any limits on this. Intuitively, the scope of what we must
set aside depends on the scope of higher-order undermining we receive,
and which instances or types of reasoning are called into question.
But how far can it go? What if
everything
is called into
question? Considering this possibility leads to paradox. On the one
hand, reason to universally doubt our reasoning seems to justify
skepticism across the board. On the other hand, the way we would
arrive at that skeptical state would be by using our reasoning…
which has been, by hypothesis, called into question. Some
epistemologists have suggested that since there is no stable and
consistent response to such cases, we ought to conclude that universal
defeat is impossible. But this response is hard to reconcile with the
fact that defeat comes on a spectrum; if the extreme cases are
impossible, what about the intermediate cases? (For further
discussion, see Egan & Elga 2005, Enoch 2010, Sliwa & Horowitz
2015, Schoenfield 2015a, Christensen 2010a, 2019. See also the entry
on
epistemic self-doubt
for further discussion.)
Even if we restrict our attention to more moderate cases of
undermining, one still might wonder whether rationality can ask us to
doubt so much that we end up skeptics about large and important
domains (even if we are not skeptics across the board). Could it be
rational for us to come to doubt, for example, all of our moral or
religious beliefs on the basis of higher-order undermining? Elga
(2007) argues (again in the context of peer disagreement) that we
should not worry about this possibility. In order to doubt all of our
moral beliefs, for example, we must have independent grounds to judge
these beliefs to be unreliable. But if we set aside all of our moral
beliefs, Elga argues, we won’t have enough left over to make any
sort of reliability judgment—and therefore, won’t have
rational grounds for doubt. So undermining can only happen locally.
(And even then, he argues, many of our religious and moral beliefs
will be safe, since on any particular occasion, the targeted religious
or moral belief will be supported by our other moral and religious
beliefs—the ones that aren’t called into question.) See
Vavova 2018 and Christensen 2011 for further discussion of this point.
Vavova and Christensen both differentiate between two possible ways to
formulate Independence: one on which you must revise your beliefs
insofar as you
fail to have
good independent reason to
trust
them, and one on which you must revise insofar as you
have
good independent reason to think that you’re
mistaken
. They argue that the first formulation has skeptical
results, while the second avoids them.
3.2 Level-bridging principles
While Independence principles tell us what to set aside,
level-bridging principles specify how our first-order beliefs should
cohere with our higher-order beliefs. (As mentioned above, however,
these principles aren’t always presented separately.
Level-bridging principles also often include some element of
Independence, or are meant to apply alongside an Independence
principle.)
Some epistemologists argue that our first-order beliefs should line up
somehow with our beliefs about
rationality
. (See Smithies
2019 and Titelbaum 2015, 2019, for example.) Others argue that our
first-order beliefs should line up somehow with our beliefs about
reliability
. And still others incorporate elements of both.
Some literature discusses both notions together and other literature
distinguishes explicitly between the two. (See Christensen 2016b,
Sliwa & Horowitz 2015, and Schoenfield 2015a for a few examples of
more explicit discussions. See also Dorst [forthcoming] for a
comprehensive overview of some of the principles proposed in the
literature and the relationship between them, and the entry on
epistemic self-doubt
for discussion of some other level-bridging principles in greater
detail.)
I will highlight the choice by first describing an intuitively
plausible level-bridging principle that focuses on rationality, and a
problem case for this principle which has motivated some
epistemologists to move towards reliability instead. Although the
second principle, too, has come under criticism, the case is a useful
illustration of some of the complications that arise in this
choice.
3.2.1 Rationality-focused principles
The general thought behind
rationality
-focused level-bridging
principles is that your beliefs about the world should line up with
your beliefs about what’s rational for you to believe. If you
think it’s rational to believe
P
(or, if it’s
rational for you to think that’s rational…) you should
believe
P
. If you think it’s rational to believe
~
P
, you should believe ~
P
. This line of thought
involves deferring to rationality as you would defer to an expert.
What if you’re uncertain about what’s rational? Arguably,
this is Aisha’s situation: she’s unsure whether her
evidence rationally supports
G
, as she initially thought it
did, or whether hypoxia has confused her and her evidence
doesn’t support
G
after all. One initially plausible
response to this sort of situation is to say that when we’re
uncertain, our beliefs should reflect a kind of weighted average of
the responses we take to be possibly rational. This yields the result
that if we’re about equally confident that we should believe
P
and that we should believe ~
P
, the thing to do is
to suspend judgment.
The principle Rational Reflection makes this thought more precise in a
degreed-belief, or “credence” setting. (Christensen
[2010b] introduces and discusses the principle at length. Salow [2018]
and Skipper [2021] are proponents of the principle. Elga [2013] and
Dorst [2020] amend it, as we’ll see later.) Roughly, according
to Rational Reflection, one’s credences (“Cr” in the
formulation below) should align with those that one regards as
rational (“Pr”):
Rational Reflection:
\(\Cr(A\mid \Pr(A) = n) =
n\)
If this principle were a true rational requirement, then rational
agents in situations like Aisha’s would end up with a strong
coherence between their first-order beliefs and their beliefs about
what is rational. Rational Reflection says, for instance, that it
cannot be rational for Aisha to be rationally
certain
that it
is rational to have .9 confidence in
G
, without
also
having .9 confidence in
G
. And it cannot be rational for
Aisha to be uncertain, without also adopting a weighted average of the
credences that she regards as possibly rational (weighted by her
credence, in each, that it is rational). In the version of the case
discussed in the introduction, one might interpret Aisha’s
higher-order evidence as indicating that her high confidence in
G
is only 50% likely to be rational, given her first-order
evidence. If it is 50% likely that high confidence is rational, and
50% likely that low confidence is rational, Aisha should plausibly
adopt the weighted average of these possibilities and arrive at a
middling level of credence. Rational Reflection supports this
explanation of the story.
However, Rational Reflection has counterintuitive consequences in some
cases, such as the “Unmarked Clock” case, introduced in
Williamson (2014). Here is that case (based on the presentation in
Christensen 2010b; see also Elga 2013; see Horowitz 2014 for a similar
case, discussed in connection with epistemic akrasia, and see also
Sliwa & Horowitz 2015 for a non-perceptual case with similar
features):
The Unmarked Clock:
Chloe is looking at an unmarked
clock with just a minute hand, which jumps discretely between its
positions. The hand is pointing somewhere around where the 4 would be,
if it were marked, which of course it isn’t. Chloe is wondering:
is the hand pointing to the 19? The 20? The 21? (call those
propositions “P19”, “P20”, and
“P21”).
Williamson, Christensen, and Elga (among others) all agree on the
following about this case:
(1)
Chloe shouldn’t be certain of any of P19, P20, or P21.
(2)
Chloe’s credence in whichever of these propositions is
actually her evidence should be highest, and should taper off as
possibilities become more remote. (In other words, if the hand is
really at 20 minutes after the hour, she should have highest credence
in P20, a bit lower credence in P19 and P21, and lower credence still
in P18 and P22.)
And finally,
(3)
Chloe can figure out all of these facts about her epistemic
situation simply by thinking about the setup of the case, as we just
have.
The problem is that (1), (2), and (3) together entail that Chloe
should violate Rational Reflection.
Here is why: suppose the hand is at 20 minutes after the hour, and
Chloe’s credence is in fact distributed as it should be, with
P20 receiving the highest value. Since Chloe is rational, then by
(2)
above, she should also give significant credence to P19 and P21. Now
consider the implications for what Chloe thinks her credence
should
be. In P20, Chloe’s current credence is
rational. But in P19 and P21 (which we have just said she gives
significant credence to), her current credence in P20 is too high. So
she is in a position to conclude the following about her credence in
P20: it’s definitely not too low, but it may well be too high.
This unbalanced state violates Rational Reflection, and also looks
like an instance of epistemic akrasia.
Adam Elga (2013) argues that this sort of situation motivates a
different principle, which he calls “New Rational
Reflection” (following Ned Hall’s “New Principal
Principle”, and the argument in favor of it; see Hall 1994).
Elga argues that just as we should not in general defer (directly) to
experts when we know more than they do, we should not in general defer
to rationality in cases where it is rational to doubt one’s
rationality. If we use “Cr” for an agent’s credence,
and “Pr” for a candidate ideally rational credence, we can
express Elga’s principle as follows:
New Rational Reflection:
\(\Cr(A\mid \textrm{Pr is
ideal}) = \Pr(A\mid \textrm{Pr is ideal})\)
Both Rational Reflection and New Rational Reflection have us defer to
rationality as we would defer to an expert. But whereas Rational
Reflection has us defer to rationality as an expert with
exactly
our evidence
, New Rational Reflection has us defer to rationality
as an expert who has our evidence
and is certain that it is an
expert
. Elga argues that this is the right way to defer to
experts more generally, making New Rational Reflection (on his view)
simply a more carefully-formulated expert deference principle. (See
Pettigrew & Titelbaum 2014 for a concurrent view.)
In the clock case, New Rational Reflection also allows for epistemic
akrasia: Chloe’s beliefs about the clock come apart from her
estimate of what’s rational for her to believe about the clock.
But New Rational Reflection offers us an explanation for why this is
unproblematic. In normal cases,
what’s rational
is a
good guide to
what’s true
: if it’s rational to
believe it will rain, it’s also likely to be true that it will
rain. But in the clock case, this generality doesn’t hold. We
can predict that rationality and truth will come apart. So when
Chloe’s belief matches her best estimate of what’s true,
it will diverge from her best estimate of what’s rational.
Furthermore, this divergence comes about because of uncertainty about
what’s rational. So by eliminating that uncertainty—by
deferring to rationality only on the condition that rationality is not
uncertain about its own expertise—New Rational Reflection aims
to eliminate that gap between rationality and truth. (See Elga 2013,
as well as Horowitz 2014, for further discussion of this feature of
the case.)
Several authors agree with this line of thought, accepting both the
setup of the clock case and Elga’s general line of thought
regarding how it should be treated. These authors take the clock case
to be one in which epistemic akrasia is rationally permissible after
all, showing that any anti-akrasia norms must be formulated carefully.
For similar discussion, see Horowitz’s (2014) discussion of the
dartboard case (which is modeled after the unmarked clock). There,
Horowitz argues that epistemic akrasia is rationally permissible when
we expect the evidence to be “falsity-guiding”; though see
Weatherson (2019: ch. 10), and Hawthorne, Isaacs, and Lasonen-Aarnio
(2021) for arguments that this condition is too narrow. Sliwa and
Horowitz (2015) present an alternative level-bridging principle,
“Evidential Calibration”, and argue that, like New
Rational Reflection, it can help differentiate between rational and
irrational cases of epistemic akrasia, as well as rule out
bootstrapping. Christensen (2016a) proposes what he calls the
“Idealized Thermometer Model”, and makes similar points in
its favor. Dorst (2020; see also his 2019) proposes another principle,
“Trust”, which is a weakening of Rational Reflection.
Dorst argues that Trust allows us to be uncertain about rationality,
while still treating rationality as an expert; this vindicates the
idea that we should defer to our evidence, and (following I. J. Good)
that more evidence is always epistemically beneficial. All of these
level-bridging principles take higher-order evidence into account, but
also allow some instances of epistemic akrasia.
If we agree with this line of thought, we might draw two lessons.
First: treating rationality as an expert is a complicated job, and any
rational expert deference principle must be formulated carefully. If
we can predict that rationality and truth will come apart, we should
not defer unrestrictedly to rationality. And relatedly, we might take
cases like this to show that epistemic akrasia can be rational: it can
crop up even on some views according to which higher-order evidence is
epistemically significant.
Some epistemologists disagree with these lessons, and moreover, with
the entire setup of the clock case. One camp of detractors objects to
claims
(1)
,
(2)
, and
(3)
as set out above. These epistemologists argue that we cannot be
rationally uncertain about what our evidence is (as Chloe is) or about
what it supports. See, for example, Stalnaker (2009), Smithies (2019:
ch. 11), and Skipper (2021). Salow (2018) argues that the conception
of evidence required to get the puzzle going can be used to allow
“biased inquiry”. Cases like the unmarked clock thus
present a choice point for epistemologists who want to accommodate
higher-order defeat: accept that such cases are possible and reject
Rational Reflection, or reject such cases and adopt a strong
transparency requirement about evidence and rationality.
Others disagree from the opposite side: Maria Lasonen-Aarnio (2015)
argues that even the modified principle, New Rational Reflection,
presupposes too much self-knowledge. She advocates rejecting all
level-bridging principles, at least as far as our theory of evidential
support is concerned (though see
section 4
for discussion of her view on cases like Aisha’s).)
Finally, Christensen (2021a) argues that New Rational Reflection
doesn’t go far enough. New Rational Reflection says we should
defer, to some extent, to
any
theory of rationality we think
might possibly be right. But what if we think it’s possible that
rationality can come apart from the truth—as it does in the
clock case, or even more broadly, as it does on views that allow moral
encroachment? (On the extreme end, consider views on which rationality
is about believing what makes you happy.) Surely we should not defer
to them, even a little bit. (If one of them is right, of course, we
should
follow
it—but the present question is just about
whether we should treat different candidates for rationality as
experts.) So Christensen argues we should reject New Rational
Reflection, and focus on attaining beliefs we take to be
accurate
, rather than beliefs we take to be rational. This
may mean moving away entirely from rationality-focused level-bridging
principles.
3.2.2 Reliability-focused principles
So far we have focused on Rational Reflection and New Rational
Reflection, which are motivated by the thought that in accommodating
higher-order evidence, we should calibrate our first-order beliefs to
our higher-order beliefs about
rationality
. Another way to
approach the question begins with
reliability
. The thought
here is that higher-order evidence affects our first-order beliefs
because of its bearing on our reliability—that is, our
propensity to get to the
truth
, under the relevant
circumstances. Examples of this approach include Elga’s
“Equal Weight View”, quoted above; Christensen
(2007a)’s “Integration”; the “Calibration
Rule”, discussed by White (2009); Weatherson’s
(ms—see Other Internet Resources) “Judgments Screen
Evidence” (which he argues against); “Guess
Calibration”, discussed in Sliwa and Horowitz 2015;
“Calibration”, discussed in Schoenfield 2015a; and the
“Simple Thermometer Model” discussed in Christensen 2016a.
As a representative example, here is White’s “Calibration
Rule”, which he presents as a consequence of the Equal Weight
View:
Calibration Rule:
If I draw the conclusion
p
on the basis of any evidence
e
, my credence in
p
should equal my prior expected reliability with respect to
p
.
There are various complications involved in interpreting this rule
(for instance, it refers to “drawing a conclusion”, which
is an all-or-nothing judgment, as well as credences). But notice that,
unlike Rational Reflection, the Calibration Rule makes no reference to
rationality
. (It does specify that the conclusion is drawn on
the basis of some evidence, but as White points out in his discussion
of the rule, the evidence plays no part in determining what we should
believe.)
Reliability-based level-bridging principles must also be formulated
carefully in order to navigate worries in the vicinity of the
generality problem. (Among other things, one’s reliability
estimate on a particular occasion must take base rates into account;
see Isaacs 2021.) But just as we saw before, such principles are
always put forth in conjunction with—or incorporate some element
of—Independence principles. If we can answer the question of how
to circumscribe Independence, presumably that will answer the
generality problem in this domain as well.
Focusing on reliability, rather than rationality, has advantages and
disadvantages. An important advantage is that this approach allows us
to accommodate cases like the unmarked clock, discussed in the
previous section. But the disadvantage is that reliability-based
level-bridging principles do not answer certain questions which, for
many epistemologists, lie at the heart of the higher-order evidence
debate: can epistemic akrasia be rational? And should we in some sense
treat rationality as an expert to which we should defer? If our
level-bridging principle does not mention rationality, it will not
have immediate consequences for these questions, which may seem
unsatisfying to some.
Distinguishing between rationality-focused and reliability-focused
level-bridging principles highlights another choice point for
epistemologists. Some take questions about akrasia to be central, and
defend rationality-based level-bridging principles for that reason
(Smithies 2019 is a paradigmatic example; see also Neta 2019).
Others—even those who support level-bridging—take cases
like the unmarked clock to show that these questions aren’t so
central after all, and that perhaps epistemic akrasia is not so
significant (Christensen 2016a and 2021a are paradigmatic examples of
this approach). A third strategy is to say that anti-akrasia, or
level-coherence principles, have a special sort of normative status
that differs from other rational requirements. Some of the proposals
discussed in the next section defend this view.
4. Dilemmas and Two-Norm Views
We began by discussing a puzzle raised by higher-order evidence: it
seems that agents in situations like Hypoxia should reduce confidence,
but also that if they do, they will be ignoring evidence in a
problematic way. We have seen some arguments against reducing
confidence, and some proposals for
how
Aisha should reduce
confidence. But some epistemologists think that neither of these
possibilities gives us the full story. Maybe level-bridging principles
are
genuine rational requirements, but there is still
something wrong with Aisha if she reduces confidence. Or maybe they
are not genuine rational requirements, yet there is something
else
wrong with Aisha if she does
not
reduce
confidence. Maybe they are both rational requirements, and Aisha is
facing an epistemic dilemma. (An alternative way to frame the puzzle
focuses on epistemic akrasia: maybe Aisha is rationally required to be
epistemically akratic in her situation, but there is something else
wrong with her if she is epistemically akratic, and so forth.)
This section discusses two strategies which acknowledge the remaining
puzzle and attempt to solve it. One, defended most prominently by
David Christensen, holds that higher-order evidence gives rise to
epistemic dilemmas, where there is no fully rational response
available (though there may be a rationally best response). So Aisha
is both required to believe
G
and to doubt
G
, and
she is just unfortunately unable to do both. Another family of
responses, here called “two-norm views”, aim to separate
different modes of epistemic evaluation, and argue that Aisha’s
conflicting requirements somehow issue from different normative
realms. The common goal of all these strategies is to explain the
apparent conflict of norms in cases like Hypoxia, without denying the
legitimacy of any of these conflicting norms.
4.1 Dilemmas
Christensen takes the puzzles surrounding higher-order evidence to
show that the requirements of rationality sometimes conflict with one
another, putting agents like Aisha in an epistemic bind. This proposal
differs from the two-norm views discussed below, in that there is just
one notion of epistemic rationality at work. According to the dilemma
view, in some circumstances, it is impossible to satisfy all the
rational requirements at once.(see Christensen 2007a, 2010a, 2013,
2016b, and 2021c). In his more recent work (see Christensen 2021c),
Christensen sees the norms in conflict, in Aisha’s case, as one
requiring us to believe what our evidence entails, and another
requiring us to revise in light of higher-order evidence. (An
alternative possibility, which Christensen suggests in earlier work
[see his 2013], is that the second norm is explicitly an anti-akratic
norm.)
An important feature of Christensen’s view is that, although
Aisha is subject to conflicting requirements, there is still a
best
epistemic response in her situation. (Christensen holds
that the best response is to reduce confidence.) This raises a
question: what determines which response is best? See Leonard 2020 for
arguments against the dilemma view; Leonard defends a nearby view
according to which there are conflicting norms, but it’s
indeterminate
(among a restricted set of possibilities) what
Aisha should believe. Knoks (2021) defends a view along similar lines,
arguing that Aisha’s situation is permissive (again, among a
similarly-restricted set of possibilities). The possibility of
dilemmas in epistemology raises several questions: how can epistemic
requirements relate (or not) to notions of epistemic blame, and how
can they guide our beliefs? See Hughes 2019 and 2021, e.g., for
further discussion of these issues (though mostly focused on a
different putative source of conflicting requirements); Hughes defends
the position that we should accept epistemic dilemmas.
4.2 Ideal vs. non-ideal modes of evaluation
A few authors have suggested that while higher-order evidence has
genuine normative significance, this significance belongs only to some
non-ideal
normative realm. So while a rationally ideal
agent’s higher-order evidence would have no effect on her
first-order beliefs, a non-ideal agent’s evidence should have
such an effect—precisely because she is non-ideal.
Joshua DiPaolo (2019) develops one version of this view. Drawing on
work in political philosophy, he argues that we need an
epistemological “theory of the second best”. On his
proposed view, the norms that apply to ideal agents are different from
the norms that apply to non-ideal agents. While the ideal norms define
a standard of perfection for all of us, non-ideal norms tell us how to
approach that standard, taking our imperfections into account. DiPaolo
argues that this approach allows us to resolve the apparent tensions
involved in accommodating higher-order evidence: ideal rationality
requires ignoring higher-order evidence, but non-ideal rationality
requires respecting it.
While non-ideal agents are limited in their reasoning abilities, they
are also limited in their abilities to double-check and
“police” themselves. Appealing to this consideration,
Joshua Schechter (2013) makes a suggestion that cuts in the opposite
direction from DiPaolo’s: he argues that perhaps our epistemic
imperfections cap our responsibility to respond to higher-order
evidence. This means that while our available evidence may call for
extensive belief revision—and while a more ideal agent
would
revise her beliefs in response to higher-order
evidence—non-ideal believers like us are permitted to stop when
we’ve done what we can.
Declan Smithies (2019) also defends a view on which the
ideal/non-ideal distinction comes into play. This is discussed in more
detail below.
4.3 Best plans to follow vs. best plans to make
Another kind of two-norm view comes from Miriam Schoenfield (see her
2015b and 2018). Schoenfield interprets judgments about rationality,
in cases like Aisha’s, as
plans
. She points out that we
can evaluate plans in (at least) two different ways: by looking at
what will happen if they are followed perfectly, and by looking at
what will happen if they are made. If you need to leave the house at
10:30, but are habitually running late, “leave the house at
10” might be the best plan to make, even if “leave the
house at 10:30” is the best plan to follow. This is similar to
DiPaolo’s suggestion in that the difference between the best
plan to make and the best plan to follow depends on a person’s
(predicted) rational shortcomings: the best plan to make is one that
takes these shortcomings into account in a particular way. However,
Schoenfield’s view is
not
that higher-order evidence
only has rational import for non-ideal agents; rather, it has the
import it has for anyone who
rationally believes
that they
are non-ideal.
An advantage of this approach is that it allows us to focus on
accuracy as the primary target of epistemic rationality. Schoenfield
frames both modes of evaluation in terms of accuracy: specifically,
the accuracy-related
consequences
of the plan. So if she is
right that the best-plan-to-make/best-plan-to-follow distinction can
explain higher-order evidence, we would end up with a relatively
uniform two-norm view. See Horowitz 2019 for two objections: first,
that focusing on consequences seems to open the door to non-epistemic
plans, such as “have a sandwich before reasoning”; second,
that the best plan to make might differ among different believers, in
which case this strategy would not yield anything like a general
defense of revising in response to higher-order evidence. This second
worry also raises questions about DiPaolo’s proposal and how
universal we can expect a non-ideal rationality to be.
4.4 Reasons vs. rationality
Alex Worsnip (2018) argues that we can explain the oddness of
higher-order evidence by distinguishing between two kinds of epistemic
requirements:
evidence responsiveness
and
coherence.
If Aisha maintains her belief that
G
,
and
believes
that her evidence may well not support this belief, she will believe
everything her evidence supports. If she revises her belief that she
has enough fuel (aligning it with her estimate of what the evidence
supports) she will be coherent. This view is one that explicitly
appeals to epistemic akrasia, as Worsnip’s coherence norms are
effectively anti-akrasia norms.
4.5 Evidence vs. dispositions
Maria Lasonen-Aarnio (2020) argues that the conflict comes from two
different modes of evaluation which we can apply to Aisha’s
case. In addition to evaluating whether believers have correctly
followed the epistemic requirements that apply to them, or have
correctly accommodated their evidence, we can also evaluate their
belief-forming dispositions. If Aisha holds onto her belief about
G
, while recognizing the danger of hypoxia in her own case,
she is responding appropriately to her evidence and is (if her belief
is formed in the right way) following the requirements that apply to
her. But she is also manifesting a disposition that is unlikely to
serve her well in the long term: ignoring what
appears
to be
a conclusive reason for belief (her belief that she’s likely to
be hypoxic, and therefore likely to be responding inappropriately to
her evidence) will often involve
actual
conclusive reasons
for belief. We can criticize Aisha for manifesting this disposition,
even if this particular case is not one of the situations in which it
goes wrong. (A relevant comparison here is Rule versus Act
Utilitarianism: a good rule might occasionally lead to imperfect
outcomes, whereas a good outcome might occasionally be the result of a
bad rule. See Coates [2012] for a similar suggestion about rules in
this context.)
Smithies (2019) defends a similar view, discussed in more detail in
the next subsection.
4.5 Propositional justification vs. doxastic justification
Finally, Paul Silva (2017) and Declan Smithies (2019) each
argue—though in different ways—that we can resolve the
conflict by distinguishing between propositional and doxastic
justification. (Also see Ye 2020 for objections to the move to
doxastic justification in this context.) Let us consider Silva’s
proposal first. An agent like Aisha, he argues, is
propositionally
justified in believing
G
, and in
believing that her evidence may well not support
G
. But she
cannot come to
rationally hold
the belief that
G
: in
other words, she cannot be
doxastically
justified in
believing
G
. Silva’s argument for this claim is in part
an inference to the best explanation: although Aisha’s evidence
supports
G
, it seems irrational for Aisha to act on that
belief, suggesting that Aisha lacks knowledge of
G
. If
Aisha’s belief is propositionally justified but not known, this
in turn suggests that the belief is not doxastically justified. (See
also van Wietmarschen 2013 for a similar proposal in the case of peer
disagreement. Van Wietmarschen’s argument does not appeal to
knowledge; rather, he argues directly that an agent’s belief is
not well-grounded, or doxastically justified, if her basis for holding
it does not respond to her higher-order evidence.)
Smithies (2019) invokes this distinction to a slightly different end.
As mentioned above, he argues that we can never be propositionally
justified in believing falsehoods about what our evidence
supports—so, Aisha is propositionally justified in believing
G
,
and
propositionally justified in believing that
her evidence supports this. But assuming that Aisha is a normal,
non-ideal believer, her own doxastic dispositions won’t be
sensitive enough to the evidence to safely track her propositional
justification. This means that she can’t be doxastically
justified in believing that her evidence supports
G
.
Interestingly, Smithies argues that in this case a non-ideal agent
like Aisha
should
be epistemically akratic; the ideal,
non-akratic state isn’t available to her, and an akratic state
is, he argues, the best she can do.
5. Conclusion
We began by noting that higher-order evidence gives rise to a puzzle.
It seems that “higher-order evidence”—information
about our own irrationality or unreliability—should prompt us to
revise our beliefs about the world, but if we do so, we must ignore
evidence and reasoning that is directly relevant to the truth of those
beliefs. Much of the literature on higher-order evidence revolves
around this puzzle: arguing that we should reject one side of the
puzzle or the other, or finding ways to hold onto both.
As of this writing, two related questions emerge as central to the
debate going forward. First: is higher-order evidence significant
because it gives us information about our own
rationality
? Or
because it gives us information about our own
reliability
?
And second: are there genuine norms prohibiting epistemic akrasia?
(So, should our beliefs about what’s
rational
line up
with our beliefs about the world?) Or does the appearance of these
norms merely arise because of nearby norms regarding
reliability—or, perhaps, because there are multiple, conflicting
modes of epistemic normativity? Untangling the puzzles surrounding
higher-order evidence will ultimately involve answering these
questions as well, as well as broader questions about the relationship
between rationality and truth.
Bibliography
Alexander, David, 2013, “The Problem of Respecting
Higher-Order Doubt”,
Philosopher’s Imprint
,
13(September): article 18 (12 pages).
 [
Alexander 2013 available online
]
Arsenault, Michael and Zachary C. Irving, 2012, “Aha! Trick
Questions, Independence, and the Epistemology of Disagreement: Trick
Questions, Independence, and the Epistemology”,
Thought: A
Journal of Philosophy
, 1(3): 185–194.
doi:10.1002/tht3.27
Barnett, Zach, 2019, “Philosophy Without Belief”,
Mind
, 128(509): 109–138. doi:10.1093/mind/fzw076
Bradley, Darren, 2019, “Are There Indefeasible Epistemic
Rules?”,
Philosopher’s Imprint
, 19: article 3 (19
pages).
 [
Bradley 2019 available online
]
Brown, Jessica, 2018,
Fallibilism: Evidence and
Knowledge
, Oxford: Oxford University Press.
doi:10.1093/oso/9780198801771.001.0001
Christensen, David, 2007a, “Does Murphy’s Law Apply in
Epistemology?”, in
Oxford Studies in Epistemology, Volume
2
, Tamar Szabó Gendler and John Hawthorne (eds.),
Oxford/New York: Oxford University Press, 3–31.
–––, 2007b, “Epistemology of Disagreement:
The Good News”,
The Philosophical Review
, 116(2):
187–217. doi:10.1215/00318108-2006-035
–––, 2010a, “Higher-Order Evidence”,
Philosophy and Phenomenological Research
, 81(1):
185–215. doi:10.1111/j.1933-1592.2010.00366.x
–––, 2010b, “Rational Reflection”,
Philosophical Perspectives
, 24(1): 121–140.
doi:10.1111/j.1520-8583.2010.00187.x
–––, 2011, “Disagreement,
Question-Begging, and Epistemic Self-Criticism”,
Philosopher’s Imprint
, 11(March): article 6 (22 pages).
 [
Christensen 2011 available online
]
–––, 2013, “Epistemic Modesty
Defended”, in Christensen and Lackey 2013: 77–97.
doi:10.1093/acprof:oso/9780199698370.003.0005
–––, 2016a, “Disagreement, Drugs, Etc.:
From Accuracy to Akrasia”,
Episteme
, 13(4):
397–422. doi:10.1017/epi.2016.20
–––, 2016b, “Conciliation, Uniqueness and
Rational Toxicity”,
Noûs
, 50(3): 584–603.
doi:10.1111/nous.12077
–––, 2018, “On Acting as Judge in
One’s Own (Epistemic) Case”,
Proceedings and Addresses
of the American Philosophical Association
, 93(1):
207–235.
–––, 2019, “Formulating
Independence”, in Skipper and Steglich-Petersen 2019:
13–34. doi:10.1093/oso/9780198829775.003.0001
–––, 2021a, “Akratic (Epistemic)
Modesty”,
Philosophical Studies
, 178(7):
2191–2214. doi:10.1007/s11098-020-01536-6
–––, 2021b, “Rationality for the
Self-Aware”,
Proceedings and Addresses of the American
Philosophical Association
, 95(1): 213–236.
–––, 2021c, “Embracing Epistemic
Dilemmas”, in McCain, Stapleford, and Steup 2021: chap. 10.
Christensen, David and Jennifer Lackey (eds.), 2013,
The
Epistemology of Disagreement: New Essays
, Oxford: Oxford
University Press.
Coates, Allen, 2012, “Rational Epistemic Akrasia”,
American Philosophical Quarterly
, 49(2): 113–124.
DiPaolo, Joshua, 2018, “Higher-Order Defeat Is
Object-Independent”,
Pacific Philosophical Quarterly
,
99(2): 248–269. doi:10.1111/papq.12155
–––, 2019, “Second Best Epistemology:
Fallibility and Normativity”,
Philosophical Studies
,
176(8): 2043–2066. doi:10.1007/s11098-018-1110-y
Dorst, Kevin, 2019, “Higher-Order Uncertainty”, in
Skipper and Steglich-Petersen 2019: 35–61.
doi:10.1093/oso/9780198829775.003.0002
–––, 2020, “Evidence: A Guide for the
Uncertain”,
Philosophy and Phenomenological Research
,
100(3): 586–632. doi:10.1111/phpr.12561
–––, forthcoming, “Higher-Order
Evidence”, in
The Routledge Handbook for the Philosophy of
Evidence
, Maria Lasonen-Aarnio and Clayton Littlejohn (eds),
Routledge.
Egan, Andy and Adam Elga, 2005, “I Can’t Believe
I’m Stupid”,
Philosophical Perspectives
, 19(1):
77–93. doi:10.1111/j.1520-8583.2005.00054.x
Elga, Adam, 2007, “Reflection and Disagreement”,
Noûs
, 41(3): 478–502.
doi:10.1111/j.1468-0068.2007.00656.x
–––, 2010, “How to Disagree about How to
Disagree”, in Feldman and Warfield 2010: 175–186.
doi:10.1093/acprof:oso/9780199226078.003.0008
–––, 2013, “The Puzzle of the Unmarked
Clock and the New Rational Reflection Principle”,
Philosophical Studies
, 164(1): 127–139.
doi:10.1007/s11098-013-0091-0
Enoch, David, 2010, “Not Just a Truthometer: Taking Oneself
Seriously (but Not Too Seriously) in Cases of Peer
Disagreement”,
Mind
, 119(476): 953–997.
doi:10.1093/mind/fzq070
Feldman, Richard, 2005, “Respecting the Evidence”,
Philosophical Perspectives
, 19(1): 95–119.
doi:10.1111/j.1520-8583.2005.00055.x
–––, 2006, “Reasonable Religious
Disagreements”, in
Philosophers Without Gods: Meditations on
Atheism and the Secular Life
, Louise Antony (ed.), Oxford/New
York: Oxford University Press, 194–214.
Feldman, Richard and Ted A. Warfield (eds.), 2010,
Disagreement
, Oxford/New York: Oxford University Press.
doi:10.1093/acprof:oso/9780199226078.001.0001
Field, Claire, 2019, “It’s Ok to Make Mistakes:
Against the Fixed Point Thesis”,
Episteme
, 16(2):
175–185. doi:10.1017/epi.2017.33
Field, Hartry, 2000, “Apriority as an Evaluative
Notion”, in
New Essays on the A Priori
, Paul Boghossian
and Christopher Peacocke (eds.), Oxford/New York: Oxford University
Press, 117–149. doi:10.1093/0199241279.003.0006
Fleisher, Will, 2021, “How to Endorse
Conciliationism”,
Synthese
, 198(10): 9913–9939.
doi:10.1007/s11229-020-02695-z
Goldberg, Sanford, 2013, “Defending Philosophy in the Face
of Systematic Disagreement”, in
Disagreement and
Skepticism
, Diego E. Machuca (ed.), New York: Routledge,
277–294.
Hall, Ned, 1994, “Correcting The Guide to Objective
Chance”,
Mind
, 103(412): 505–518.
doi:10.1093/mind/103.412.505
Hawthorne, John, Yoaav Isaacs, and Maria Lasonen‐Aarnio,
2021, “The Rationality of Epistemic Akrasia”,
Philosophical Perspectives
, 35(1): 206–228.
doi:10.1111/phpe.12144
Hedden, Brian and Kevin Dorst, forthcoming, “(Almost) All
Evidence is Higher-Order Evidence”,
Analysis
, early
online: 8 May 2022. doi:10.1093/analys/anab081
Henderson, Leah, forthcoming, “Higher‐order Evidence
and Losing One’s Conviction”,
Noûs
, early
online: 3 May 2021. doi:10.1111/nous.12367
Horowitz, Sophie, 2014, “Epistemic Akrasia”,
Noûs
, 48(4): 718–744. doi:10.1111/nous.12026
–––, 2019, “Predictably Misleading
Evidence”, in Skipper and Steglich-Petersen 2019: 105–123.
doi:10.1093/oso/9780198829775.003.0005
Hughes, Nick, 2019, “Dilemmic Epistemology”,
Synthese
, 196(10): 4059–4090.
doi:10.1007/s11229-017-1639-x
–––, 2021, “Who’s Afraid of
Epistemic Dilemmas?”, in McCain, Stapleford, and Steup 2021:
chap. 15.
Isaacs, Yoaav, 2021, “The Fallacy of Calibrationism”,
Philosophy and Phenomenological Research
, 102(2):
247–260. doi:10.1111/phpr.12640
Kelly, Thomas, 2005, “The Epistemic Significance of
Disagreement”, in
Oxford Studies in Epistemology, Volume
1
, Tamar Szabó Gendler and John Hawthorne (eds.),
Oxford/New York: Oxford University Press, 167–196.
–––, 2010, “Peer Disagreement and
Higher‐Order Evidence”, in Feldman and Warfield 2010:
111–174. doi:10.1093/acprof:oso/9780199226078.003.0007
–––, 2013, “Disagreement and the Burdens
of Judgment”, in Christensen and Lackey 2013: 31–53.
doi:10.1093/acprof:oso/9780199698370.003.0003
Knoks, Aleks, 2021, “Misleading Higher-Order Evidence,
Conflicting Ideals, and Defeasible Logic”,
Ergo an Open
Access Journal of Philosophy
, 8(6): 141–174.
doi:10.3998/ergo.1143
Lasonen-Aarnio, Maria, 2014, “Higher-Order Evidence and the
Limits of Defeat”,
Philosophy and Phenomenological
Research
, 88(2): 314–345. doi:10.1111/phpr.12090
–––, 2015, “New Rational Reflection and
Internalism about Rationality”, in
Oxford Studies in
Epistemology, Volume 5
, Tamar Szabó Gendler and John
Hawthorne (eds.), Oxford/New York: Oxford University Press,
145–171. doi:10.1093/acprof:oso/9780198722762.003.0005
–––, 2020, “Enkrasia or Evidentialism?
Learning to Love Mismatch”,
Philosophical Studies
,
177(3): 597–632. doi:10.1007/s11098-018-1196-2
Leonard, Nick, 2020, “Epistemic Dilemmas and Rational
Indeterminacy”,
Philosophical Studies
, 177(3):
573–596. doi:10.1007/s11098-018-1195-3
Lewis, David, 1971, “Immodest Inductive Methods”,
Philosophy of Science
, 38(1): 54–63.
doi:10.1086/288339
Littlejohn, Clayton, 2018, “Stop Making Sense? On a Puzzle
about Rationality”,
Philosophy and Phenomenological
Research
, 96(2): 257–272. doi:10.1111/phpr.12271
Lord, Errol, 2014, “From Independence to Conciliationism: An
Obituary”,
Australasian Journal of Philosophy
, 92(2):
365–377. doi:10.1080/00048402.2013.829506
McCain, Kevin, Scott Stapleford, and Matthias Steup (eds), 2021,
Epistemic Dilemmas: New Arguments, New Angles
, New York:
Routledge. doi:10.4324/9781003134565
Neta, Ram, 2019, “The Puzzles of Easy Knowledge and of
Higher-Order Evidence: A Unified Solution”, in Skipper and
Steglich-Petersen 2019: 173–188.
doi:10.1093/oso/9780198829775.003.0008
Pettigrew, Richard and Michael G. Titelbaum, 2014,
“Deference Done Right”,
Philosopher’s
Imprint
, 14(December): article 35 (19 pages).
 [
Pettigrew and Titelbaum 2014 available online
]
Roush, Sherrilyn, 2009, “Second Guessing: A Self-Help
Manual”,
Episteme
, 6(3): 251–268.
doi:10.3366/E1742360009000690
–––, 2017, “Epistemic Self-Doubt”,
The Stanford Encyclopedia of Philosophy
(Winter 2017
Edition), Edward N. Zalta (ed.), URL =
 <
https://plato.stanford.edu/archives/win2017/entries/epistemic-self-doubt/
>.
Salow, Bernhard, 2018, “The Externalist’s Guide to
Fishing for Compliments”,
Mind
, 127(507):
691–728. doi:10.1093/mind/fzw029
Schoenfield, Miriam, 2015a, “A Dilemma for
Calibrationism”,
Philosophy and Phenomenological
Research
, 91(2): 425–455. doi:10.1111/phpr.12125
–––, 2015b, “Bridging Rationality and
Accuracy”:,
The Journal of Philosophy
, 112(12):
633–657. doi:10.5840/jphil20151121242
–––, 2018, “An Accuracy Based Approach to
Higher Order Evidence”,
Philosophy and Phenomenological
Research
, 96(3): 690–715. doi:10.1111/phpr.12329
Schechter, Joshua, 2013, “Rational Self-Doubt and the
Failure of Closure”,
Philosophical Studies
, 163(2):
429–452. doi:10.1007/s11098-011-9823-1
Silva, Paul, 2017, “How Doxastic Justification Helps Us
Solve the Puzzle of Misleading Higher-Order Evidence”,
Pacific Philosophical Quarterly
, 98(S1): 308–328.
doi:10.1111/papq.12173
–––, 2018, “Explaining Enkratic
Asymmetries: Knowledge-First Style”,
Philosophical
Studies
, 175(11): 2907–2930.
doi:10.1007/s11098-017-0987-1
Skipper, Mattias, 2021, “Does Rationality Demand
Higher-Order Certainty?”,
Synthese
, 198(12):
11561–11585. doi:10.1007/s11229-020-02814-w
Skipper, Mattias and Asbjørn Steglich-Petersen (eds.),
2019,
Higher-Order Evidence: New Essays
, Oxford/New York:
Oxford University Press. doi:10.1093/oso/9780198829775.001.0001
Sliwa, Paulina and Sophie Horowitz, 2015, “Respecting All
the Evidence”,
Philosophical Studies
, 172(11):
2835–2858. doi:10.1007/s11098-015-0446-9
Smithies, Declan, 2019,
The Epistemic Role of
Consciousness
, (Philosophy of Mind Series), New York, NY: Oxford
University Press. doi:10.1093/oso/9780199917662.001.0001
Staffel, Julia, forthcoming, “Transitional Attitudes and the
Unmooring View of Higher‐order Evidence”,
Noûs
, early online: 6 October 2021.
doi:10.1111/nous.12400
Stalnaker, Robert C., 2009, “On Hawthorne and Magidor on
Assertion, Context, and Epistemic Accessibility”,
Mind
,
118(470): 399–409. doi:10.1093/mind/fzp061
Steglich-Petersen, Asbjørn, 2019, “Higher-Order
Defeat and Doxastic Resilience”, in Skipper and
Steglich-Petersen 2019: 209–225.
doi:10.1093/oso/9780198829775.003.0010
Tal, Eyal, 2021, “Is Higher-Order Evidence Evidence?”,
Philosophical Studies
, 178(10): 3157–3175.
doi:10.1007/s11098-020-01574-0
Titelbaum, Michael G., 2015, “Rationality’s Fixed
Point (or: In Defense of Right Reason)”, in
Oxford Studies
in Epistemology, Volume 5
, Tamar Szabó Gendler and John
Hawthorne (eds.), Oxford/New York: Oxford University Press,
253–294. doi:10.1093/acprof:oso/9780198722762.003.0009
–––, 2019, “Return to Reason”, in
Skipper and Steglich-Petersen 2019: 226–245.
doi:10.1093/oso/9780198829775.003.0011
Vavova, Katia, 2018, “Irrelevant Influences”,
Philosophy and Phenomenological Research
, 96(1):
134–152. doi:10.1111/phpr.12297
Weatherson, Brian, 2013, “Disagreements, Philosophical, and
Otherwise”, in Christensen and Lackey 2013: 54–73.
doi:10.1093/acprof:oso/9780199698370.003.0004
–––, 2019,
Normative Externalism
,
Oxford: Oxford University Press.
doi:10.1093/oso/9780199696536.001.0001
Wedgwood, Ralph, 2012, “Justified Inference”,
Synthese
, 189(2): 273–295.
doi:10.1007/s11229-011-0012-8
White, Roger, 2010, “You Just Believe That
Because…”,
Philosophical Perspectives
, 24(1):
573–615. doi:10.1111/j.1520-8583.2010.00204.x
–––, 2009, “On Treating Oneself and Others
as Thermometers”,
Episteme
, 6(3):
233–250. doi:10.3366/e1742360009000689
van Wietmarschen, Han, 2013, “Peer Disagreement, Evidence,
and Well-Groundedness”,
The Philosophical Review
,
122(3): 395–425. doi:10.1215/00318108-2087654
Williamson, Timothy, 2014, “Very Improbable
Knowing”,
Erkenntnis
, 79(5): 971–999.
doi:10.1007/s10670-013-9590-9
Worsnip, Alex, 2018, “The Conflict of Evidence and
Coherence”,
Philosophy and Phenomenological
Research
, 96(1): 3–44. doi:10.1111/phpr.12246
Ye, Ru, 2020, “Higher-Order Defeat and Intellectual
Responsibility”,
Synthese
, 197(12): 5435–5455.
doi:10.1007/s11229-018-01972-2
Academic Tools
How to cite this entry
.
Preview the PDF version of this entry
at the
Friends of the SEP Society
.
Look up topics and thinkers related to this entry
at the Internet Philosophy Ontology Project (InPhO).
Enhanced bibliography for this entry
at
PhilPapers
, with links to its database.
Other Internet Resources
Elga, Adam, 2008,
 “
Lucky to be Rational
”,
 paper presented on 6 June 2008 at the Bellingham Summer Philosophy
Conference.
Weatherson Brian, ms
 “
Do Judgments Screen Evidence?
”
 unpublished draft manuscript dated 14 April 2010.
Related Entries
disagreement
|
evidence
|
justification, epistemic: coherentist theories of
|
justification, epistemic: foundationalist theories of
|
justification, epistemic: internalist vs. externalist conceptions of
|
rational choice, normative: expected utility
|
rational choice, normative: rivals to expected utility
|
rationality
|
rationality: structural
|
reliabilist epistemology
|
self-doubt, epistemic
epistemic-self-doubt
Acknowledgments
I would like to thank David Christensen for helpful comments and
suggestions.